{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('./wili dataset/x_train.txt', header=None)\n",
    "X_train = df[[0]]\n",
    "df = pd.read_fwf('./wili dataset/x_test.txt', header=None)\n",
    "X_test = df[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_fwf('./wili dataset/y_train.txt',header = None)\n",
    "y_train = target[0]\n",
    "target = pd.read_fwf('./wili dataset/y_test.txt',header = None)\n",
    "y_test = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klement Gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Klement Gottwaldi surnukeha palsameeriti ning ...\n",
       "1  Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...\n",
       "2  à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...\n",
       "3  AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...\n",
       "4  à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ne l fin de l seclo XIX l Japon era inda Ã§con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiedam is gelegen tussen Rotterdam en Vlaard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Halukum adalah kelenjar tiroid nang menonjol d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Ne l fin de l seclo XIX l Japon era inda Ã§con...\n",
       "1  Schiedam is gelegen tussen Rotterdam en Vlaard...\n",
       "2  Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...\n",
       "3  à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...\n",
       "4  Halukum adalah kelenjar tiroid nang menonjol d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    est\n",
       "1    swe\n",
       "2    mai\n",
       "3    oci\n",
       "4    tha\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train == 'be-tara'] = 'be-tarask'\n",
    "y_train[y_train == 'roa-tar'] = 'roa-tara'\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./wili dataset/labels.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>English</th>\n",
       "      <th>Wiki Code</th>\n",
       "      <th>ISO 369-3</th>\n",
       "      <th>German</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Writing system</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ace</td>\n",
       "      <td>Achinese</td>\n",
       "      <td>ace</td>\n",
       "      <td>ace</td>\n",
       "      <td>Achinesisch</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>af</td>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>als</td>\n",
       "      <td>Alemannic German</td>\n",
       "      <td>als</td>\n",
       "      <td>gsw</td>\n",
       "      <td>Alemannisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ursprünglich nur Elsässisch)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amh</td>\n",
       "      <td>Amharic</td>\n",
       "      <td>am</td>\n",
       "      <td>amh</td>\n",
       "      <td>Amharisch</td>\n",
       "      <td>Afro-Asiatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ang</td>\n",
       "      <td>Old English</td>\n",
       "      <td>ang</td>\n",
       "      <td>ang</td>\n",
       "      <td>Altenglisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ca. 450-1100)</td>\n",
       "      <td>Angelsächsisch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label           English Wiki Code ISO 369-3       German Language family  \\\n",
       "0   ace          Achinese       ace       ace  Achinesisch    Austronesian   \n",
       "1   afr         Afrikaans        af       afr    Afrikaans   Indo-European   \n",
       "2   als  Alemannic German       als       gsw  Alemannisch   Indo-European   \n",
       "3   amh           Amharic        am       amh    Amharisch    Afro-Asiatic   \n",
       "4   ang      Old English        ang       ang  Altenglisch   Indo-European   \n",
       "\n",
       "  Writing system                        Remarks        Synonyms  \n",
       "0            NaN                            NaN             NaN  \n",
       "1            NaN                            NaN             NaN  \n",
       "2            NaN  (ursprünglich nur Elsässisch)             NaN  \n",
       "3            NaN                            NaN             NaN  \n",
       "4            NaN                 (ca. 450-1100)  Angelsächsisch  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {}\n",
    "counter = 0\n",
    "for label in labels['Label']:\n",
    "    if label not in label2int:\n",
    "        label2int[label] = counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the  target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = []\n",
    "for label in y_train:\n",
    "    y_train_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_int = []\n",
    "for label in y_test:\n",
    "    y_test_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char',min_df=25,lowercase=True, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=25,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2int = vectorizer.transform(X_train[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2int = vectorizer.transform(X_test[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=80, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train = pca.transform(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_test = pca.transform(X_test2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train2int, y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.93      0.79       500\n",
      "          1       0.96      0.56      0.71       500\n",
      "          2       0.90      0.43      0.58       500\n",
      "          3       0.99      0.98      0.99       500\n",
      "          4       0.95      0.89      0.92       500\n",
      "          5       0.70      0.92      0.79       500\n",
      "          6       0.87      0.47      0.61       500\n",
      "          7       0.82      0.51      0.63       500\n",
      "          8       0.51      0.94      0.66       500\n",
      "          9       0.59      0.70      0.64       500\n",
      "         10       0.88      0.71      0.78       500\n",
      "         11       0.96      0.90      0.93       500\n",
      "         12       0.77      0.93      0.84       500\n",
      "         13       0.93      0.99      0.96       500\n",
      "         14       0.85      0.92      0.89       500\n",
      "         15       0.42      0.63      0.51       500\n",
      "         16       0.73      0.69      0.71       500\n",
      "         17       0.00      0.00      0.00       500\n",
      "         18       0.50      0.86      0.63       500\n",
      "         19       0.96      0.05      0.09       500\n",
      "         20       0.45      0.01      0.02       500\n",
      "         21       0.60      0.70      0.65       500\n",
      "         22       0.99      0.99      0.99       500\n",
      "         23       0.65      0.27      0.38       500\n",
      "         24       0.76      0.78      0.77       500\n",
      "         25       0.65      0.92      0.76       500\n",
      "         26       0.84      0.81      0.82       500\n",
      "         27       0.73      0.86      0.79       500\n",
      "         28       0.90      0.31      0.46       500\n",
      "         29       0.54      0.18      0.27       500\n",
      "         30       0.98      0.95      0.96       500\n",
      "         31       0.85      0.74      0.79       500\n",
      "         32       0.92      0.94      0.93       500\n",
      "         33       0.97      0.94      0.96       500\n",
      "         34       0.99      0.99      0.99       500\n",
      "         35       0.96      0.95      0.96       500\n",
      "         36       0.88      0.99      0.93       500\n",
      "         37       0.93      0.89      0.91       500\n",
      "         38       0.85      0.39      0.53       500\n",
      "         39       0.91      0.89      0.90       500\n",
      "         40       1.00      0.95      0.97       500\n",
      "         41       0.92      0.97      0.95       500\n",
      "         42       0.32      0.94      0.47       500\n",
      "         43       0.78      0.35      0.48       500\n",
      "         44       0.97      0.85      0.91       500\n",
      "         45       0.51      1.00      0.67       500\n",
      "         46       0.82      0.91      0.86       500\n",
      "         47       0.63      0.02      0.05       500\n",
      "         48       0.90      0.94      0.92       500\n",
      "         49       0.87      0.99      0.93       500\n",
      "         50       0.31      0.53      0.39       500\n",
      "         51       0.81      0.82      0.82       500\n",
      "         52       0.68      0.74      0.71       500\n",
      "         53       0.66      0.98      0.79       500\n",
      "         54       0.84      0.70      0.76       500\n",
      "         55       0.89      0.73      0.80       500\n",
      "         56       0.58      0.91      0.71       500\n",
      "         57       0.88      0.95      0.91       500\n",
      "         58       0.60      0.69      0.64       500\n",
      "         59       0.93      0.71      0.81       500\n",
      "         60       0.90      0.89      0.89       500\n",
      "         61       0.89      0.89      0.89       500\n",
      "         62       0.95      0.68      0.79       500\n",
      "         63       0.90      0.96      0.93       500\n",
      "         64       0.93      0.96      0.94       500\n",
      "         65       0.67      0.85      0.75       500\n",
      "         66       0.95      0.36      0.52       500\n",
      "         67       0.75      0.97      0.85       500\n",
      "         68       0.90      0.98      0.94       500\n",
      "         69       0.99      0.94      0.96       500\n",
      "         70       0.95      0.94      0.95       500\n",
      "         71       0.72      0.99      0.84       500\n",
      "         72       0.98      0.86      0.92       500\n",
      "         73       0.42      0.45      0.44       500\n",
      "         74       0.98      0.98      0.98       500\n",
      "         75       0.94      0.72      0.81       500\n",
      "         76       0.18      0.97      0.30       500\n",
      "         77       0.39      0.36      0.37       500\n",
      "         78       0.93      0.77      0.84       500\n",
      "         79       0.96      0.96      0.96       500\n",
      "         80       0.96      0.98      0.97       500\n",
      "         81       0.98      0.71      0.82       500\n",
      "         82       0.63      0.73      0.68       500\n",
      "         83       0.81      0.32      0.46       500\n",
      "         84       0.75      0.88      0.81       500\n",
      "         85       0.28      0.89      0.42       500\n",
      "         86       0.50      0.01      0.03       500\n",
      "         87       0.80      0.98      0.88       500\n",
      "         88       0.78      0.42      0.55       500\n",
      "         89       0.90      0.89      0.90       500\n",
      "         90       0.83      0.36      0.50       500\n",
      "         91       0.82      1.00      0.90       500\n",
      "         92       0.92      0.98      0.95       500\n",
      "         93       0.92      0.96      0.94       500\n",
      "         94       0.77      0.91      0.84       500\n",
      "         95       0.50      1.00      0.66       500\n",
      "         96       0.52      0.98      0.68       500\n",
      "         97       0.93      0.98      0.95       500\n",
      "         98       0.64      0.98      0.77       500\n",
      "         99       0.98      0.88      0.93       500\n",
      "        100       0.95      0.79      0.87       500\n",
      "        101       0.76      0.57      0.65       500\n",
      "        102       0.74      0.12      0.21       500\n",
      "        103       0.76      0.40      0.52       500\n",
      "        104       0.54      0.62      0.58       500\n",
      "        105       0.99      0.99      0.99       500\n",
      "        106       0.82      0.95      0.88       500\n",
      "        107       0.85      0.90      0.88       500\n",
      "        108       0.89      0.98      0.93       500\n",
      "        109       0.94      0.69      0.79       500\n",
      "        110       0.98      0.85      0.91       500\n",
      "        111       0.72      0.82      0.77       500\n",
      "        112       0.83      0.97      0.89       500\n",
      "        113       0.82      0.95      0.88       500\n",
      "        114       0.99      0.76      0.86       500\n",
      "        115       0.66      0.78      0.72       500\n",
      "        116       0.94      0.85      0.89       500\n",
      "        117       0.97      0.88      0.92       500\n",
      "        118       0.61      0.80      0.69       500\n",
      "        119       0.92      0.52      0.66       500\n",
      "        120       0.99      0.87      0.93       500\n",
      "        121       0.98      0.45      0.62       500\n",
      "        122       0.95      0.94      0.94       500\n",
      "        123       0.82      0.91      0.87       500\n",
      "        124       0.96      0.05      0.10       500\n",
      "        125       0.99      0.99      0.99       500\n",
      "        126       0.96      0.05      0.10       500\n",
      "        127       0.70      0.19      0.30       500\n",
      "        128       0.71      0.88      0.78       500\n",
      "        129       0.98      0.50      0.67       500\n",
      "        130       0.90      0.76      0.82       500\n",
      "        131       0.98      0.50      0.66       500\n",
      "        132       0.91      0.99      0.95       500\n",
      "        133       0.89      0.97      0.93       500\n",
      "        134       0.93      0.48      0.63       500\n",
      "        135       0.90      0.99      0.95       500\n",
      "        136       0.70      0.98      0.82       500\n",
      "        137       0.33      0.63      0.44       500\n",
      "        138       0.71      0.86      0.78       500\n",
      "        139       1.00      1.00      1.00       500\n",
      "        140       0.79      0.14      0.24       500\n",
      "        141       0.81      0.74      0.78       500\n",
      "        142       0.89      0.97      0.93       500\n",
      "        143       0.86      0.68      0.76       500\n",
      "        144       1.00      1.00      1.00       500\n",
      "        145       0.78      0.63      0.70       500\n",
      "        146       0.75      0.79      0.77       500\n",
      "        147       0.47      0.65      0.55       500\n",
      "        148       0.74      0.04      0.08       500\n",
      "        149       0.90      0.90      0.90       500\n",
      "        150       0.76      0.41      0.53       500\n",
      "        151       0.65      0.37      0.47       500\n",
      "        152       0.76      0.19      0.30       500\n",
      "        153       0.79      0.87      0.83       500\n",
      "        154       0.90      0.87      0.88       500\n",
      "        155       0.57      0.59      0.58       500\n",
      "        156       0.76      0.87      0.81       500\n",
      "        157       0.97      0.96      0.97       500\n",
      "        158       0.92      0.83      0.87       500\n",
      "        159       0.46      0.99      0.63       500\n",
      "        160       0.78      0.72      0.75       500\n",
      "        161       0.46      0.30      0.36       500\n",
      "        162       0.91      0.99      0.95       500\n",
      "        163       0.63      0.67      0.65       500\n",
      "        164       0.95      0.32      0.48       500\n",
      "        165       0.65      0.52      0.58       500\n",
      "        166       0.70      0.87      0.77       500\n",
      "        167       0.73      0.94      0.82       500\n",
      "        168       0.90      0.97      0.94       500\n",
      "        169       0.87      0.74      0.80       500\n",
      "        170       0.82      0.87      0.84       500\n",
      "        171       0.92      0.91      0.92       500\n",
      "        172       0.90      0.85      0.88       500\n",
      "        173       0.75      0.87      0.81       500\n",
      "        174       0.87      0.97      0.92       500\n",
      "        175       1.00      0.01      0.02       500\n",
      "        176       0.92      0.86      0.89       500\n",
      "        177       0.68      0.56      0.61       500\n",
      "        178       0.99      0.79      0.88       500\n",
      "        179       0.77      0.94      0.85       500\n",
      "        180       0.62      0.94      0.74       500\n",
      "        181       0.30      0.66      0.42       500\n",
      "        182       0.94      0.99      0.97       500\n",
      "        183       0.97      0.97      0.97       500\n",
      "        184       0.96      0.87      0.91       500\n",
      "        185       0.70      0.80      0.75       500\n",
      "        186       0.94      0.86      0.90       500\n",
      "        187       0.76      0.88      0.82       500\n",
      "        188       0.91      0.97      0.94       500\n",
      "        189       0.78      0.95      0.86       500\n",
      "        190       0.83      0.25      0.38       500\n",
      "        191       0.91      0.93      0.92       500\n",
      "        192       0.84      0.87      0.85       500\n",
      "        193       0.58      0.94      0.72       500\n",
      "        194       0.94      0.69      0.79       500\n",
      "        195       0.82      0.91      0.87       500\n",
      "        196       0.44      0.89      0.59       500\n",
      "        197       0.93      0.88      0.91       500\n",
      "        198       0.93      0.82      0.87       500\n",
      "        199       0.97      0.97      0.97       500\n",
      "        200       0.97      0.99      0.98       500\n",
      "        201       0.99      0.68      0.81       500\n",
      "        202       0.00      0.00      0.00       500\n",
      "        203       0.99      0.96      0.97       500\n",
      "        204       0.97      0.70      0.82       500\n",
      "        205       0.89      0.97      0.93       500\n",
      "        206       0.80      0.74      0.77       500\n",
      "        207       0.89      0.99      0.93       500\n",
      "        208       0.98      0.98      0.98       500\n",
      "        209       0.85      0.88      0.86       500\n",
      "        210       0.92      0.99      0.95       500\n",
      "        211       0.84      0.91      0.87       500\n",
      "        212       0.59      0.89      0.71       500\n",
      "        213       0.88      0.65      0.74       500\n",
      "        214       0.78      0.96      0.86       500\n",
      "        215       0.49      0.98      0.65       500\n",
      "        216       0.92      0.54      0.68       500\n",
      "        217       0.82      0.99      0.90       500\n",
      "        218       0.74      0.88      0.80       500\n",
      "        219       0.94      0.86      0.90       500\n",
      "        220       0.97      0.98      0.98       500\n",
      "        221       0.88      0.46      0.60       500\n",
      "        222       0.89      0.99      0.93       500\n",
      "        223       0.87      0.95      0.91       500\n",
      "        224       0.85      0.97      0.91       500\n",
      "        225       0.84      0.95      0.89       500\n",
      "        226       0.90      0.91      0.90       500\n",
      "        227       0.79      0.79      0.79       500\n",
      "        228       0.86      0.92      0.89       500\n",
      "        229       1.00      0.12      0.21       500\n",
      "        230       0.78      0.97      0.87       500\n",
      "        231       0.90      0.71      0.79       500\n",
      "        232       0.70      0.57      0.63       500\n",
      "        233       0.83      0.60      0.70       500\n",
      "        234       0.79      0.82      0.80       500\n",
      "\n",
      "avg / total       0.80      0.76      0.75    117500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_int, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7626297872340425"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_train_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = np.array(y_train_int).reshape(-1, 1)\n",
    "y_test_int = np.array(y_test_int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = enc.transform(y_train_int)\n",
    "y_test_enc = enc.transform(y_test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=80, units=70)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=70)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=70)`\n",
      "  \"\"\"\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"sigmoid\", units=235)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "clf.add(Dense(output_dim = 70, kernel_initializer = 'uniform', activation='relu', input_dim = 80))\n",
    "clf.add(Dropout(rate=0.1))\n",
    "clf.add(Dense(output_dim = 70, kernel_initializer = 'uniform', activation='relu'))\n",
    "clf.add(Dropout(rate=0.2))\n",
    "clf.add(Dense(output_dim = 70, kernel_initializer = 'uniform', activation='relu'))\n",
    "clf.add(Dropout(rate=0.1))\n",
    "clf.add(Dense(output_dim =235  , kernel_initializer = 'uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117500 samples, validate on 117500 samples\n",
      "Epoch 1/10\n",
      "117500/117500 [==============================] - 14s 117us/step - loss: 3.6820 - acc: 0.1026 - val_loss: 2.6494 - val_acc: 0.2653\n",
      "Epoch 2/10\n",
      "117500/117500 [==============================] - 13s 112us/step - loss: 2.5724 - acc: 0.2687 - val_loss: 2.0441 - val_acc: 0.4159\n",
      "Epoch 3/10\n",
      "117500/117500 [==============================] - 13s 112us/step - loss: 2.1094 - acc: 0.3830 - val_loss: 1.6810 - val_acc: 0.5175\n",
      "Epoch 4/10\n",
      "117500/117500 [==============================] - 13s 112us/step - loss: 1.8033 - acc: 0.4677 - val_loss: 1.4216 - val_acc: 0.5905\n",
      "Epoch 5/10\n",
      "117500/117500 [==============================] - 13s 112us/step - loss: 1.5782 - acc: 0.5325 - val_loss: 1.2447 - val_acc: 0.6453\n",
      "Epoch 6/10\n",
      "117500/117500 [==============================] - 13s 113us/step - loss: 1.4266 - acc: 0.5778 - val_loss: 1.1351 - val_acc: 0.6728\n",
      "Epoch 7/10\n",
      "117500/117500 [==============================] - 13s 113us/step - loss: 1.3165 - acc: 0.6103 - val_loss: 1.0456 - val_acc: 0.6970\n",
      "Epoch 8/10\n",
      "117500/117500 [==============================] - 13s 113us/step - loss: 1.2420 - acc: 0.6322 - val_loss: 0.9852 - val_acc: 0.7208\n",
      "Epoch 9/10\n",
      "117500/117500 [==============================] - 13s 113us/step - loss: 1.1785 - acc: 0.6514 - val_loss: 0.9274 - val_acc: 0.7402\n",
      "Epoch 10/10\n",
      "117500/117500 [==============================] - 13s 113us/step - loss: 1.1279 - acc: 0.6690 - val_loss: 0.8907 - val_acc: 0.7492\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(pca_X_train, y_train_enc, batch_size=48, epochs=10, validation_data=(pca_X_test,y_test_enc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
