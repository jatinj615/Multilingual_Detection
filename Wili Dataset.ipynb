{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('./wili dataset/x_train.txt', header=None)\n",
    "X_train = df[[0]]\n",
    "df = pd.read_fwf('./wili dataset/x_test.txt', header=None)\n",
    "X_test = df[[0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_fwf('./wili dataset/y_train.txt',header = None)\n",
    "y_train = target[0]\n",
    "target = pd.read_fwf('./wili dataset/y_test.txt',header = None)\n",
    "y_test = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klement Gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Klement Gottwaldi surnukeha palsameeriti ning ...\n",
       "1  Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...\n",
       "2  à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...\n",
       "3  AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...\n",
       "4  à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ne l fin de l seclo XIX l Japon era inda Ã§con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiedam is gelegen tussen Rotterdam en Vlaard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Halukum adalah kelenjar tiroid nang menonjol d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Ne l fin de l seclo XIX l Japon era inda Ã§con...\n",
       "1  Schiedam is gelegen tussen Rotterdam en Vlaard...\n",
       "2  Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...\n",
       "3  à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...\n",
       "4  Halukum adalah kelenjar tiroid nang menonjol d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    est\n",
       "1    swe\n",
       "2    mai\n",
       "3    oci\n",
       "4    tha\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train == 'be-tara'] = 'be-tarask'\n",
    "y_train[y_train == 'roa-tar'] = 'roa-tara'\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./wili dataset/labels.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>English</th>\n",
       "      <th>Wiki Code</th>\n",
       "      <th>ISO 369-3</th>\n",
       "      <th>German</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Writing system</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ace</td>\n",
       "      <td>Achinese</td>\n",
       "      <td>ace</td>\n",
       "      <td>ace</td>\n",
       "      <td>Achinesisch</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>af</td>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>als</td>\n",
       "      <td>Alemannic German</td>\n",
       "      <td>als</td>\n",
       "      <td>gsw</td>\n",
       "      <td>Alemannisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ursprünglich nur Elsässisch)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amh</td>\n",
       "      <td>Amharic</td>\n",
       "      <td>am</td>\n",
       "      <td>amh</td>\n",
       "      <td>Amharisch</td>\n",
       "      <td>Afro-Asiatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ang</td>\n",
       "      <td>Old English</td>\n",
       "      <td>ang</td>\n",
       "      <td>ang</td>\n",
       "      <td>Altenglisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ca. 450-1100)</td>\n",
       "      <td>Angelsächsisch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label           English Wiki Code ISO 369-3       German Language family  \\\n",
       "0   ace          Achinese       ace       ace  Achinesisch    Austronesian   \n",
       "1   afr         Afrikaans        af       afr    Afrikaans   Indo-European   \n",
       "2   als  Alemannic German       als       gsw  Alemannisch   Indo-European   \n",
       "3   amh           Amharic        am       amh    Amharisch    Afro-Asiatic   \n",
       "4   ang      Old English        ang       ang  Altenglisch   Indo-European   \n",
       "\n",
       "  Writing system                        Remarks        Synonyms  \n",
       "0            NaN                            NaN             NaN  \n",
       "1            NaN                            NaN             NaN  \n",
       "2            NaN  (ursprünglich nur Elsässisch)             NaN  \n",
       "3            NaN                            NaN             NaN  \n",
       "4            NaN                 (ca. 450-1100)  Angelsächsisch  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {}\n",
    "counter = 0\n",
    "for label in labels['Label']:\n",
    "    if label not in label2int:\n",
    "        label2int[label] = counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the  target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = []\n",
    "for label in y_train:\n",
    "    y_train_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_int = []\n",
    "for label in y_test:\n",
    "    y_test_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char',min_df=25,lowercase=True, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=25,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2int = vectorizer.transform(X_train[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2int = vectorizer.transform(X_test[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.fit(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2int_sc = sc.transform(X_train2int)\n",
    "X_test2int_sc = sc.transform(X_test2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=80, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train2int_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train = pca.transform(X_train2int_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_test = pca.transform(X_test2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train2int, y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.91      0.87       500\n",
      "          1       0.84      0.78      0.81       500\n",
      "          2       0.43      0.69      0.53       500\n",
      "          3       0.99      0.98      0.99       500\n",
      "          4       0.92      0.90      0.91       500\n",
      "          5       0.81      0.75      0.78       500\n",
      "          6       0.80      0.50      0.61       500\n",
      "          7       0.74      0.80      0.77       500\n",
      "          8       0.36      0.85      0.50       500\n",
      "          9       0.46      0.75      0.57       500\n",
      "         10       0.61      0.76      0.68       500\n",
      "         11       0.98      0.86      0.92       500\n",
      "         12       0.84      0.86      0.85       500\n",
      "         13       0.94      0.99      0.96       500\n",
      "         14       0.88      0.93      0.90       500\n",
      "         15       0.82      0.37      0.51       500\n",
      "         16       0.88      0.54      0.67       500\n",
      "         17       0.82      0.08      0.15       500\n",
      "         18       0.64      0.28      0.39       500\n",
      "         19       0.90      0.04      0.07       500\n",
      "         20       0.78      0.68      0.73       500\n",
      "         21       0.75      0.60      0.67       500\n",
      "         22       0.98      0.99      0.99       500\n",
      "         23       0.88      0.10      0.18       500\n",
      "         24       0.54      0.98      0.69       500\n",
      "         25       0.86      0.90      0.88       500\n",
      "         26       0.94      0.72      0.81       500\n",
      "         27       0.70      0.87      0.77       500\n",
      "         28       0.62      0.60      0.61       500\n",
      "         29       0.53      0.15      0.23       500\n",
      "         30       0.99      0.95      0.97       500\n",
      "         31       0.91      0.70      0.79       500\n",
      "         32       0.91      0.95      0.93       500\n",
      "         33       0.89      0.96      0.92       500\n",
      "         34       0.99      0.99      0.99       500\n",
      "         35       0.57      0.96      0.71       500\n",
      "         36       0.70      0.99      0.82       500\n",
      "         37       0.90      0.90      0.90       500\n",
      "         38       0.88      0.60      0.71       500\n",
      "         39       0.95      0.83      0.89       500\n",
      "         40       1.00      0.95      0.97       500\n",
      "         41       0.88      0.98      0.93       500\n",
      "         42       0.45      0.91      0.60       500\n",
      "         43       0.57      0.67      0.62       500\n",
      "         44       0.97      0.85      0.91       500\n",
      "         45       0.89      1.00      0.94       500\n",
      "         46       0.97      0.80      0.88       500\n",
      "         47       0.00      0.00      0.00       500\n",
      "         48       0.88      0.95      0.92       500\n",
      "         49       0.86      0.99      0.92       500\n",
      "         50       0.34      0.55      0.42       500\n",
      "         51       0.87      0.78      0.82       500\n",
      "         52       0.70      0.72      0.71       500\n",
      "         53       0.80      0.97      0.88       500\n",
      "         54       0.93      0.56      0.70       500\n",
      "         55       0.97      0.42      0.59       500\n",
      "         56       0.68      0.87      0.76       500\n",
      "         57       0.79      0.96      0.87       500\n",
      "         58       0.78      0.39      0.52       500\n",
      "         59       0.86      0.76      0.81       500\n",
      "         60       0.84      0.89      0.86       500\n",
      "         61       0.99      0.83      0.90       500\n",
      "         62       0.94      0.69      0.80       500\n",
      "         63       0.93      0.95      0.94       500\n",
      "         64       0.93      0.96      0.95       500\n",
      "         65       0.90      0.71      0.79       500\n",
      "         66       0.95      0.36      0.52       500\n",
      "         67       0.78      0.97      0.87       500\n",
      "         68       0.92      0.98      0.95       500\n",
      "         69       0.98      0.94      0.96       500\n",
      "         70       0.94      0.94      0.94       500\n",
      "         71       0.76      0.99      0.86       500\n",
      "         72       0.85      0.94      0.89       500\n",
      "         73       0.30      0.66      0.41       500\n",
      "         74       0.99      0.30      0.46       500\n",
      "         75       0.55      0.83      0.66       500\n",
      "         76       0.71      0.05      0.09       500\n",
      "         77       0.67      0.00      0.01       500\n",
      "         78       0.87      0.88      0.88       500\n",
      "         79       0.95      0.96      0.95       500\n",
      "         80       0.96      0.97      0.97       500\n",
      "         81       0.96      0.71      0.82       500\n",
      "         82       0.74      0.62      0.68       500\n",
      "         83       0.67      0.60      0.63       500\n",
      "         84       0.85      0.86      0.86       500\n",
      "         85       0.82      0.65      0.73       500\n",
      "         86       0.54      0.20      0.30       500\n",
      "         87       0.66      0.99      0.79       500\n",
      "         88       0.65      0.76      0.70       500\n",
      "         89       0.83      0.93      0.88       500\n",
      "         90       0.74      0.51      0.60       500\n",
      "         91       0.91      1.00      0.95       500\n",
      "         92       0.93      0.98      0.96       500\n",
      "         93       0.86      0.96      0.91       500\n",
      "         94       0.84      0.91      0.88       500\n",
      "         95       0.50      1.00      0.66       500\n",
      "         96       0.99      0.48      0.65       500\n",
      "         97       0.93      0.98      0.96       500\n",
      "         98       0.73      0.97      0.84       500\n",
      "         99       0.98      0.88      0.93       500\n",
      "        100       0.89      0.84      0.86       500\n",
      "        101       1.00      0.04      0.07       500\n",
      "        102       0.18      0.56      0.27       500\n",
      "        103       0.81      0.32      0.46       500\n",
      "        104       0.63      0.57      0.60       500\n",
      "        105       0.99      0.99      0.99       500\n",
      "        106       0.95      0.91      0.93       500\n",
      "        107       0.86      0.90      0.88       500\n",
      "        108       0.90      0.98      0.93       500\n",
      "        109       0.64      0.89      0.74       500\n",
      "        110       0.98      0.85      0.91       500\n",
      "        111       0.73      0.79      0.76       500\n",
      "        112       0.86      0.97      0.91       500\n",
      "        113       0.99      0.83      0.91       500\n",
      "        114       0.98      0.80      0.88       500\n",
      "        115       0.94      0.54      0.68       500\n",
      "        116       0.90      0.85      0.88       500\n",
      "        117       0.65      0.94      0.77       500\n",
      "        118       0.65      0.82      0.73       500\n",
      "        119       0.84      0.61      0.71       500\n",
      "        120       0.99      0.86      0.92       500\n",
      "        121       0.88      0.60      0.71       500\n",
      "        122       0.96      0.92      0.94       500\n",
      "        123       0.65      0.96      0.78       500\n",
      "        124       0.96      0.21      0.34       500\n",
      "        125       0.98      0.99      0.98       500\n",
      "        126       0.68      0.26      0.38       500\n",
      "        127       0.54      0.24      0.34       500\n",
      "        128       0.96      0.53      0.69       500\n",
      "        129       0.57      0.78      0.65       500\n",
      "        130       0.70      0.97      0.81       500\n",
      "        131       0.94      0.24      0.38       500\n",
      "        132       0.90      0.99      0.95       500\n",
      "        133       0.82      0.97      0.89       500\n",
      "        134       0.94      0.67      0.78       500\n",
      "        135       0.86      0.99      0.92       500\n",
      "        136       0.88      0.97      0.92       500\n",
      "        137       0.51      0.46      0.48       500\n",
      "        138       0.84      0.82      0.83       500\n",
      "        139       1.00      1.00      1.00       500\n",
      "        140       0.44      0.67      0.53       500\n",
      "        141       0.70      0.86      0.77       500\n",
      "        142       0.96      0.95      0.96       500\n",
      "        143       0.72      0.74      0.73       500\n",
      "        144       1.00      1.00      1.00       500\n",
      "        145       0.69      0.64      0.66       500\n",
      "        146       0.68      0.80      0.74       500\n",
      "        147       0.86      0.26      0.40       500\n",
      "        148       0.26      0.95      0.41       500\n",
      "        149       0.83      0.91      0.87       500\n",
      "        150       0.78      0.56      0.65       500\n",
      "        151       0.52      0.52      0.52       500\n",
      "        152       0.77      0.28      0.41       500\n",
      "        153       0.72      0.88      0.79       500\n",
      "        154       0.78      0.90      0.84       500\n",
      "        155       0.90      0.35      0.50       500\n",
      "        156       0.90      0.75      0.82       500\n",
      "        157       0.98      0.96      0.97       500\n",
      "        158       0.92      0.92      0.92       500\n",
      "        159       0.70      0.99      0.82       500\n",
      "        160       0.78      0.73      0.75       500\n",
      "        161       0.54      0.04      0.07       500\n",
      "        162       0.94      0.99      0.97       500\n",
      "        163       0.88      0.43      0.57       500\n",
      "        164       0.66      0.57      0.61       500\n",
      "        165       0.76      0.40      0.52       500\n",
      "        166       0.69      0.87      0.77       500\n",
      "        167       0.77      0.88      0.82       500\n",
      "        168       0.89      0.98      0.93       500\n",
      "        169       0.92      0.67      0.78       500\n",
      "        170       0.86      0.84      0.85       500\n",
      "        171       0.88      0.93      0.90       500\n",
      "        172       0.61      0.92      0.73       500\n",
      "        173       0.82      0.88      0.85       500\n",
      "        174       0.79      0.98      0.87       500\n",
      "        175       0.99      0.14      0.24       500\n",
      "        176       0.93      0.84      0.88       500\n",
      "        177       0.90      0.18      0.29       500\n",
      "        178       0.96      0.90      0.92       500\n",
      "        179       0.99      0.68      0.81       500\n",
      "        180       0.77      0.86      0.81       500\n",
      "        181       0.67      0.36      0.47       500\n",
      "        182       0.98      0.99      0.98       500\n",
      "        183       0.97      0.97      0.97       500\n",
      "        184       0.91      0.88      0.89       500\n",
      "        185       0.84      0.67      0.74       500\n",
      "        186       0.84      0.87      0.85       500\n",
      "        187       0.74      0.88      0.81       500\n",
      "        188       0.85      0.98      0.91       500\n",
      "        189       0.97      0.91      0.94       500\n",
      "        190       0.35      0.78      0.48       500\n",
      "        191       0.75      0.95      0.84       500\n",
      "        192       0.62      0.93      0.74       500\n",
      "        193       0.74      0.92      0.82       500\n",
      "        194       0.49      0.92      0.64       500\n",
      "        195       0.65      0.93      0.77       500\n",
      "        196       0.53      0.88      0.66       500\n",
      "        197       0.98      0.82      0.89       500\n",
      "        198       0.89      0.84      0.86       500\n",
      "        199       0.94      0.97      0.96       500\n",
      "        200       0.98      0.99      0.99       500\n",
      "        201       0.57      0.84      0.68       500\n",
      "        202       0.00      0.00      0.00       500\n",
      "        203       0.99      0.96      0.97       500\n",
      "        204       0.57      0.81      0.67       500\n",
      "        205       0.99      0.92      0.95       500\n",
      "        206       0.73      0.78      0.75       500\n",
      "        207       0.89      0.99      0.94       500\n",
      "        208       0.98      0.98      0.98       500\n",
      "        209       0.85      0.81      0.83       500\n",
      "        210       0.95      0.98      0.97       500\n",
      "        211       0.83      0.89      0.86       500\n",
      "        212       0.70      0.87      0.78       500\n",
      "        213       1.00      0.09      0.16       500\n",
      "        214       0.93      0.96      0.95       500\n",
      "        215       0.70      0.97      0.82       500\n",
      "        216       0.93      0.52      0.66       500\n",
      "        217       0.81      0.99      0.89       500\n",
      "        218       0.84      0.88      0.86       500\n",
      "        219       0.91      0.88      0.90       500\n",
      "        220       0.97      0.98      0.98       500\n",
      "        221       0.48      0.76      0.59       500\n",
      "        222       0.87      0.99      0.93       500\n",
      "        223       0.90      0.95      0.92       500\n",
      "        224       0.52      0.99      0.68       500\n",
      "        225       0.84      0.95      0.89       500\n",
      "        226       0.79      0.92      0.85       500\n",
      "        227       0.84      0.71      0.77       500\n",
      "        228       0.94      0.92      0.93       500\n",
      "        229       0.67      0.99      0.80       500\n",
      "        230       0.58      0.98      0.73       500\n",
      "        231       0.88      0.75      0.81       500\n",
      "        232       0.77      0.46      0.58       500\n",
      "        233       0.81      0.50      0.62       500\n",
      "        234       0.81      0.79      0.80       500\n",
      "\n",
      "avg / total       0.80      0.76      0.75    117500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_int, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622978723404256"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_train_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = np.array(y_train_int).reshape(-1, 1)\n",
    "y_test_int = np.array(y_test_int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = enc.transform(y_train_int)\n",
    "y_test_enc = enc.transform(y_test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=80, units=130)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=130)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=130)`\n",
      "  \"\"\"\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"softmax\", units=235)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "clf = Sequential()\n",
    "clf.add(Dense(output_dim = 130, kernel_initializer = 'uniform', activation='relu', input_dim = 80))\n",
    "clf.add(Dropout(rate=0.2))\n",
    "clf.add(Dense(output_dim = 130, kernel_initializer = 'uniform', activation='relu'))\n",
    "clf.add(Dropout(rate=0.3))\n",
    "clf.add(Dense(output_dim = 130, kernel_initializer = 'uniform', activation='relu'))\n",
    "clf.add(Dropout(rate=0.2))\n",
    "clf.add(Dense(output_dim =235  , kernel_initializer = 'uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117500 samples, validate on 117500 samples\n",
      "Epoch 1/10\n",
      "117500/117500 [==============================] - 25s 210us/step - loss: 2.9933 - acc: 0.2187 - val_loss: 1.9743 - val_acc: 0.4265\n",
      "Epoch 2/10\n",
      "117500/117500 [==============================] - 13s 112us/step - loss: 2.0317 - acc: 0.3985 - val_loss: 1.5655 - val_acc: 0.5292\n",
      "Epoch 3/10\n",
      "117500/117500 [==============================] - 15s 128us/step - loss: 1.6723 - acc: 0.4957 - val_loss: 1.2570 - val_acc: 0.6277\n",
      "Epoch 4/10\n",
      "117500/117500 [==============================] - 15s 127us/step - loss: 1.4185 - acc: 0.5723 - val_loss: 1.0623 - val_acc: 0.6867\n",
      "Epoch 5/10\n",
      "117500/117500 [==============================] - 14s 119us/step - loss: 1.2579 - acc: 0.6230 - val_loss: 0.9500 - val_acc: 0.7263\n",
      "Epoch 6/10\n",
      "117500/117500 [==============================] - 14s 120us/step - loss: 1.1487 - acc: 0.6562 - val_loss: 0.8634 - val_acc: 0.7524\n",
      "Epoch 7/10\n",
      "117500/117500 [==============================] - 14s 122us/step - loss: 1.0712 - acc: 0.6810 - val_loss: 0.8115 - val_acc: 0.7610\n",
      "Epoch 8/10\n",
      "117500/117500 [==============================] - 13s 115us/step - loss: 1.0172 - acc: 0.6978 - val_loss: 0.7755 - val_acc: 0.7791\n",
      "Epoch 9/10\n",
      "117500/117500 [==============================] - 13s 115us/step - loss: 0.9743 - acc: 0.7125 - val_loss: 0.7404 - val_acc: 0.7900\n",
      "Epoch 10/10\n",
      "117500/117500 [==============================] - 14s 115us/step - loss: 0.9388 - acc: 0.7247 - val_loss: 0.7177 - val_acc: 0.7942\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(pca_X_train, y_train_enc, batch_size=48, epochs=10, validation_data=(pca_X_test,y_test_enc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
