{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('./wili dataset/x_train.txt', header=None)\n",
    "X_train = df[[0]]\n",
    "df = pd.read_fwf('./wili dataset/x_test.txt', header=None)\n",
    "X_test = df[[0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_fwf('./wili dataset/y_train.txt',header = None)\n",
    "y_train = target[0]\n",
    "target = pd.read_fwf('./wili dataset/y_test.txt',header = None)\n",
    "y_test = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klement Gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Klement Gottwaldi surnukeha palsameeriti ning ...\n",
       "1  Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...\n",
       "2  à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...\n",
       "3  AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...\n",
       "4  à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ne l fin de l seclo XIX l Japon era inda Ã§con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiedam is gelegen tussen Rotterdam en Vlaard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Halukum adalah kelenjar tiroid nang menonjol d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Ne l fin de l seclo XIX l Japon era inda Ã§con...\n",
       "1  Schiedam is gelegen tussen Rotterdam en Vlaard...\n",
       "2  Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...\n",
       "3  à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...\n",
       "4  Halukum adalah kelenjar tiroid nang menonjol d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    est\n",
       "1    swe\n",
       "2    mai\n",
       "3    oci\n",
       "4    tha\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train == 'be-tara'] = 'be-tarask'\n",
    "y_train[y_train == 'roa-tar'] = 'roa-tara'\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./wili dataset/labels.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>English</th>\n",
       "      <th>Wiki Code</th>\n",
       "      <th>ISO 369-3</th>\n",
       "      <th>German</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Writing system</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ace</td>\n",
       "      <td>Achinese</td>\n",
       "      <td>ace</td>\n",
       "      <td>ace</td>\n",
       "      <td>Achinesisch</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>af</td>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>als</td>\n",
       "      <td>Alemannic German</td>\n",
       "      <td>als</td>\n",
       "      <td>gsw</td>\n",
       "      <td>Alemannisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ursprünglich nur Elsässisch)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amh</td>\n",
       "      <td>Amharic</td>\n",
       "      <td>am</td>\n",
       "      <td>amh</td>\n",
       "      <td>Amharisch</td>\n",
       "      <td>Afro-Asiatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ang</td>\n",
       "      <td>Old English</td>\n",
       "      <td>ang</td>\n",
       "      <td>ang</td>\n",
       "      <td>Altenglisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ca. 450-1100)</td>\n",
       "      <td>Angelsächsisch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label           English Wiki Code ISO 369-3       German Language family  \\\n",
       "0   ace          Achinese       ace       ace  Achinesisch    Austronesian   \n",
       "1   afr         Afrikaans        af       afr    Afrikaans   Indo-European   \n",
       "2   als  Alemannic German       als       gsw  Alemannisch   Indo-European   \n",
       "3   amh           Amharic        am       amh    Amharisch    Afro-Asiatic   \n",
       "4   ang      Old English        ang       ang  Altenglisch   Indo-European   \n",
       "\n",
       "  Writing system                        Remarks        Synonyms  \n",
       "0            NaN                            NaN             NaN  \n",
       "1            NaN                            NaN             NaN  \n",
       "2            NaN  (ursprünglich nur Elsässisch)             NaN  \n",
       "3            NaN                            NaN             NaN  \n",
       "4            NaN                 (ca. 450-1100)  Angelsächsisch  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {}\n",
    "counter = 0\n",
    "for label in labels['Label']:\n",
    "    if label not in label2int:\n",
    "        label2int[label] = counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the  target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = []\n",
    "for label in y_train:\n",
    "    y_train_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_int = []\n",
    "for label in y_test:\n",
    "    y_test_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char',min_df=25,lowercase=True, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=25,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2int = vectorizer.transform(X_train[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2int = vectorizer.transform(X_test[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=80, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train = pca.transform(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8XXW57/HP0yFJM7RphobSeWYotECsTCKUWUUGRUCuB5UrVw8igucoXu8R9agHcEJ9nesVGU71IDIPcpTBClWGU2hLoaUU6JDObcY28/zcP9YKTctuuppkZ+1kf9+v136ttX7Ze/+eNLt5sn6juTsiIiL7GxZ3ACIikpqUIEREJCElCBERSUgJQkREElKCEBGRhJQgREQkISUIERFJSAlCREQSUoIQEZGERsQdQF8UFRX51KlT4w5DRGRQWb58eaW7Fx/seUlLEGZ2N/AxoNzd54ZlBcD9wFSgDPiUu9eYmQE/Bz4CNAKfdfcVB6tj6tSpLFu2LDnfgIjIEGVmm6I8L5lNTP8BnLdf2U3AYnefBSwOrwHOB2aFj2uAXyUxLhERiSBpCcLd/wZU71d8IbAoPF8EXNSt/Lce+G8g38zGJys2ERE5uIHupC5x9x0A4XFcWD4B2NLteVvDMhERiUmqjGKyBGUJ1yE3s2vMbJmZLauoqEhyWCIi6WugE8Surqaj8Fgelm8FJnV73kRge6I3cPc73L3U3UuLiw/aCS8iIr000AniCeCq8Pwq4PFu5f9ggROBPV1NUSIiEo9kDnO9DzgdKDKzrcDNwC3AA2Z2NbAZuDR8+p8IhriuIxjm+rlkxSUiItEkLUG4+xUH+NKZCZ7rwLXJikVEZLBo6+ikrrmd+uZ26lu6Hm3UNbfT0NJBQ1i28IhxzJuUn9RYBvVMahGRVNPc1sGepjb2NLVR29RGbXPXeTu1TW3UtYTH5nZqm9uobW6nrjm4rmtuo7mtM1I9xXmZShAiInHo6HR2N7ZS09hKdUMbNY2t1DS0UtPYxu7GVnY3BmW7m9rY09jG7qagrKW951/wWSOHkZc1krysEYzOGsnorBFMzB9FXtaI8BF8LTdzRHgcSU7m8H3OczJGMGxYosGf/UsJQkTSgrtT29ROZUMLVfWtVNW3UFnfQmV9K1XvlQXn1Q3BL35PONgeMkYMI3/USMZmZ5CfPZKpRdmMGTWG/OwMxowa+b7H6FF7E0LGiFSZXXBwShAiMqg1t3VQUddCeV0LFXXNVNS1dLtuoaK+hcq6IBG0drz/r3szyB81ksLcTApyMphdkkdhbgYFOZkUZI9kbE4GBTkZjM3OYGxOBmOzRzJq5HCCJeSGNiUIEUlJ7k5NYxs79jSxc08zO2ubg+OeZnbVtbBrTzO76prZ3dj2vtcOMyjMzaQ4N5PivExmjcujKC+D4txMinIzKczNeO9YkJ3BiOGD56/6gaQEISKxqG1uY1tNE9t3N7F9TzM7djexY08zO/Z0HZtp3a89f5gFnbOHjc5icmE2C6YVMC4vk3GjMxmXl0VxeF6Yk8nwAWijH+qUIESk37k7lfWtbNvdxLaaJrbWNL533nWsa2nf5zXDhxmHjc5i/Jgsjp2Yz7lHZ713fVj4KM7N1F/7A0gJQkR6pbmtg83VjWyqamRzdSNbqoPj5upGttY0vm+4Zl7WCCbkj2JC/igWTCsIzseO4vCwrChXf/WnGiUIETmgxtZ2yiob2VTVQFlVI2WVDZRVNbCpqpGdtc37PDc3cwSTC7KZWZzL6bOLmTh2FBPGZjMhfxQTC0YxOmtkTN+F9JYShEia6+h0ttY0sr6ing0VDayvaGBDRT1lVQ3sqm3Z57lFuZlMLczmlJlFTCnMZkphNpMLsplSmMPY7JFpMbInnShBiKSJuua2MAHUB4/yBjZU1lNW2bjP8M/87JFML8rh1JnFTCvKZmpRDlMLc5hSmE2e7gLSihKEyBDi7lQ1tLKuvJ53y+tZt6uOdRX1rCuv3+duYMQwY3JhNtOLcjljzjimF+cwoziX6cW5FORkxPgdSCpRghAZhNydiroW3i2v551ddWEyqOfd8jpqus0LyMkYzsxxuZw6s5gZ43KYWZzLjHG5TC7IZqRGA8lBKEGIpLi65jbe2VXH2p11vN312FW3zwSxMaNGMmtcLufNHc/McbnMGpfLzHG5jB+TpX4B6TUlCJEU0dHpbKpq4K0ddazdWfvecWtN03vPyc0cweySXM6fO57ZJbnMKcljZkkuxbmZSgTS75QgRGLQ1NrB2p21rNlRy5rtwXHtjjqa2jqAYNLYtKIc5k/K54oFk5lTksecw/KYOHaUEoEMGCUIkSSrbW7jzW21vLl9D6u37eHN7bWsr6inM1wpNC9rBEeNH83lCyZx5PjRHDV+NDPH5ZI1cni8gUvaU4IQ6UdNrR28uX0PK7fs5o2te3hj627Kqhrf+/pho7M4+vDRnD/3MI46fAxHHz5adwWSsiInCDPLcfeGZAYjMph0djrrKupZuXk3K7fuZuXm3by9q46O8NYgWFNoDJ88YSJzJ4zh6MPHUJyXGXPUItEdNEGY2cnAnUAuMNnM5gH/y93/MdnBiaSS+pZ2VmyqYfmmGlZsrmHllt3UNQcLzuVljWD+pHy+dMQM5k3KZ97EMYwbnRVzxCJ9E+UO4mfAucATAO7+upmdltSoRFJATUMrSzdWs3RjFa+WVbNmey2dHmwwM6ckjwvmHc7xk8cyf1I+04tyBmQLSJGBFKmJyd237NdG2pGccETiU9vcxtIN1by0vpL/3lDN2p21uEPmiGEcNzmfL58xk9KpBRw3OV9LTkhaiJIgtoTNTG5mGcBXgLf6UqmZXQ98ATDgN+5+u5kVAPcDU4Ey4FPuXtOXekR60tzWwYpNNbywrpIX11exautuOsOEcMKUsdx41mxOmlHIsRPzB9U+wiL9JUqC+CLwc2ACsBV4Bri2txWa2VyC5LAAaAWeMrP/CssWu/stZnYTcBPwjd7WI7I/d2ftzjr+9k4FL6yr5NWyaprbOhk+zJg/KZ9rz5jJyTOKOH5KPpkjNMRU5KAJwt0rgSv7sc4jgf9290YAM1sCXAxcCJwePmcR8DxKENJHe5raeOHdSpa8U86SdyreW7BudkkuVyyYzKkzi1gwrUBNRiIJRBnFtAi43t13h9djgZ+4++d7Wedq4AdmVgg0AR8BlgEl7r4DwN13mNm4A8RzDXANwOTJk3sZggxV7s7bu+p4bm0Fz71dzvJNNXR0OnlZIzhtVjEfnl3MabOLOWyMRhiJHEyUJqZju5IDgLvXmNlxva3Q3d8ys1uBZ4F64HWgvedX7fP6O4A7AEpLS723ccjQ0dreydKNVfxlzS7+8lY523YHaxcdNX40X/zwdE6fM47jJuVrL2ORQxQlQQwzs7FdHcZhZ3KfZmC7+13AXeH7/ZCgb2OXmY0P7x7GA+V9qUOGtvqWdp5/u5yn39zF82vLqWtpJ2vkME6dWcx1C2dyxhHjKNE8BJE+ifKL/ifAS2b2UHh9KfCDvlRqZuPcvdzMJgOXACcB04CrgFvC4+N9qUOGnuqGVv7y1i6eXr2Tv6+rpLW9k4KcDD5yzHjOPqqEU2YWMSpDncsi/SVKJ/VvzWw5cAbBsNRL3H1NH+t9OOyDaAOuDZutbgEeMLOrgc0EiUjSXHltM0+9uZM/r9rJK2XVdHQ6E/JHceUHJ3Pe0YdROrWA4ZqgJpIUUZuK1gI1Xc83s8nuvrm3lbr7hxKUVQFn9vY9Zegor23mT6t28KdVO3l1UzXuMKM4hy99eAbnzT2Mow8frcXtRAZAlFFM1wE3A7sIZlAb4MCxyQ1N0klVfQt/Xr2TJ9/YztKNQVKYXZLL9WfO4qPHjGdWSV7cIYqknSh3ENcDc8K/8EX6TVNrB8+s2cmjr23j7+9W0tHpzCjO4SsLZ3HBvPHMHKekIBKnSEttAHuSHYikB3fn1bIaHly2hT+v3kl9SzuHj8nimtOm8/F5h3PEYXlqPhJJEVESxAbg+XA5jJauQnf/adKikiFnx54mHlmxjQeXbaGsqpGcjOF85JjxXHz8BE6cVqiVUEVSUJQEsTl8ZIQPkUjaOjpZ/NYu7n91C0veqaDT4cTpBVy3cBbnH3MY2Rna0FAklUUZ5vrdgQhEho7NVY3c+8omHl6+lcr6VkpGZ3LtGTP55AkTmVKYE3d4IhJRlFFMxcDXgaOB96amuvvCJMYlg0xHp7PknXJ+9/Imnn+ngmFmLDxiHFcsmMRps4q1zIXIIBTlHv9egn0aPkaw9PdVQEUyg5LBY09TGw8u28Kil8vYUt1EcV4m1y2cxRULJjF+zKi4wxORPoiSIArd/S4zu97dlwBLwiW6JY2tK69n0UtlPLxiK42tHSyYWsA3zjuCc48+jJG6WxAZEqIkiLbwuMPMPgpsByYmLyRJVe7OC+squeuFjTz/dgUZI4Zx4bzDuerkqcydMCbu8ESkn0VJEN83szHA14BfAqOBG5IalaSU1vZOHlu5jTv/voF3dtVTlJvJjWfP5tMfnExRbmbc4YlIkkQZxfRkeLqHYME+SRN1zW3c98pm7nphI7tqWzjisDx+fOk8Lpg3XltyiqSBAyYIM/u6u99mZr8kWHtpH+7+laRGJrGpa27j7hfKuPOFDdQ1t3PyjEJu++Q8TptVpFnOImmkpzuIt8LjsoEIROLX2NrOopc28eu/rWd3YxvnHFXCtWfMZN6k/LhDE5EYHDBBuPsfzWw4MNfd/3kAY5IB1tLewe+Xbubfn1tPZX0Lp88p5sazZ3PsRCUGkXTWYx+Eu3eY2QkDFYwMrLaOTh5evpVfLH6X7XuaOXF6Af/vfxxP6dSCuEMTkRQQZRTTa2b2BPAg0NBV6O6PJC0qSarOTufJVTv42bPvsLGygfmT8vnRpfM4eUah+hhE5D1REkQBUAV0X1rDASWIQcbdef6dCn701Nus2VHLEYfl8Zt/KOWsI8cpMYjI+0QZ5vq5gQhEkmv1tj3865NrWLqxmkkFo7j9svlcMO9w7ecsIgcUZbG+LOBq3r9Y3+eTGJf0k/K6Zn789Ns8uHwrY7Mz+N6FR3P5ByaTMULLYYhIz6I0Mf0OWAucC3wPuJK9Q2AlRbW2d3L3ixv55eJ3ae3o5H+eOo0vL5zFmFEj4w5NRAaJKAliprtfamYXuvsiM/s98HRfKjWzG4D/SdCXsQr4HDAe+ANBn8cK4DPu3tqXetLVi+sq+fbjq1lf0cBZR5bwrY8eybQi7cMgIocmSjtD12J9u81sLjAGmNrbCs1sAvAVoNTd5wLDgcuBW4GfufssoIagWUsOwc49zVz7+xVceedS2judez77Ae68qlTJQUR6JcodxB1mNhb4P8ATQC7wL/1Q7ygzawOygR0Eo6Q+HX59EfAd4Fd9rCctdHY69726mVv+tJbWjk5uPHs215w2nayRWi9JRHqvp7WYStx9l7vfGRb9DZje1wrdfZuZ/Zhgn+sm4BlgObDb3dvDp20FJvS1rnSwoaKemx5ZxSsbqzl5RiH/dskx2tZTRPpFT3cQr5vZKuA+4GF339MfFYZ3IxcC04DdBBPwzk/w1PctEBi+/hrgGoDJkyf3R0iDUmenc/eLG7nt6bfJGjGM2z5xLJeWTtR8BhHpNz0liAnAWQT9A/9mZi8TJIsn3L2pD3WeBWx09woAM3sEOBnIN7MR4V3ERIKNid7H3e8A7gAoLS1NmESGuh17mvjaA6/z0voqzjqyhB9ePJdxo7MO/kIRkUPQ02J9HQSjlZ42swyCv/IvB35uZovd/cpe1rkZONHMsgmamM4kWDH2OeCTBCOZrgIe7+X7D2lPvrGd//3IKto7nVs/cQyfKp2kuwYRSYoondS4e6uZrSGY/3ACcFRvK3T3pWb2EMFQ1nbgNYI7gv8C/mBm3w/L7uptHUNRc1sH33tyDb9fupn5k/K5/bL5TNXoJBFJoh4ThJlNBi4DrgByCP66v9Dd+zRRzt1vBm7er3gDsKAv7ztUbalu5B/vXcGqbXv40ukz+NrZsxkxXDOhRSS5ehrF9BJBP8SDwDXuro2DYvDXtbu44f7X6XTnN/9QytlHlcQdkoikiZ7uIL4J/M3d07IjOG4dnc7P//IOv/jrOo4+fDS/uvIEJhdmxx2WiKSRnjqplwxkILLX7sZWrv/DSpa8U8GlJ0zkXy+aq0lvIjLgInVSy8BZvW0PX/zP5ZTXtvDDi4/higUapSQi8VCCSCHPrS3nS/cuZ2x2Bg988STmT9Ke0CISn546qW/s6YXu/tP+Dyd9Pbx8K19/+A2OHJ/HPZ9dQHFeZtwhiUia6+kOIi88zgE+QLBQH8AFBOsyST+542/r+eGf1nLyjEJ+/ZkTyMvSng0iEr+eOqm/C2BmzwDHu3tdeP0dgqGv0kfuzi1PreXXSzbw0WPG89PL5pE5Qp3RIpIaovRBTAa6b9zTSh/2g5BAZ6fz3T++yaKXN3HlByfzvQvnan9oEUkpUbccfcXMHiVYYfVi4LdJjWqI6+x0vvXYKu57ZQtf+NA0/vdHjtRIJRFJOQdNEO7+AzP7M/ChsOhz7v5acsMaujo6nX9+6HUeWbGNL58xk6+dM1vJQURSUtRhrtlArbvfY2bFZjbN3TcmM7ChqKPTufGBlTy+cjtfO3s21505K+6QREQO6KAJwsxuBkoJRjPdA4wE/hM4JbmhDS2dnc43Hn6Dx1du5xvnHcGXTp8Rd0giIj2KsiToxcDHgQYAd9/O3iGwEoG7863HVvPQ8q3ccNZsJQcRGRSiJIjWcME+BzAzbUJwCNyd7zzxJve9splrz5jBV86cGXdIIiKRREkQD5jZrwm2BP0C8BfgN8kNa+j4xeJ1LHp5E1/40DT+6Zw56pAWkUEjyiimH5vZ2UAtQT/Et9392aRHNgT8de0ufvaXd7jk+Akayioig07ULUefBZQUDsGmqga++oeVHDV+ND+8+BglBxEZdA7axGRml5jZu2a2x8xqzazOzGoHIrjBqqm1gy/+5wrMjF9/5gTt5SAig1KUO4jbgAv6ug91unB3vvXoKtburOXuz36ASQXaBU5EBqcondS7lByie+L17Tzy2ja+euZszpgzLu5wRER6LcodxDIzux94DGjpKnT3R5IW1SDV3NbBrX9eyzETxnDdQg1nFZHBLUqCGA00Aud0K3OgVwnCzOYA93crmg58m2ABwPsJVootAz7l7jW9qSMud7+4ke17mvnJp+YzTCuzisggF2WY6+f6s0J3fxuYD2Bmw4FtwKPATcBid7/FzG4Kr7/Rn3UnU1V9C//3ufWcdWQJJ80ojDscEZE+62nL0a+7+21m9kvCWdTduftX+qH+M4H17r7JzC4ETg/LFwHPM4gSxM8Xv0tTWwc3nX9E3KGIiPSLnu4gujqmlyWx/suB+8LzEnffAeDuO8xs0PTwrq+o596lm/n0gsnMHJcbdzgiIv2ipy1H/xgeFyWjYjPLIFgE8JuH+LprgGsAJk+enITIDt0tf17LqJHDuf4sLd8tIkNHlOW+iwmaeo4CsrrK3X1hH+s+H1jh7rvC611mNj68exgPlCd6kbvfAdwBUFpa+r6mr4H2xtbdPLtmF/987hyKcjPjDkdEpN9EmQdxL0Fz0zTguwQjjF7th7qvYG/zEsATwFXh+VXA4/1QR9Ld/+oWskYO4zMnTYk7FBGRfhUlQRS6+11Am7svcffPAyf2pVIzywbOZt+hsrcAZ5vZu+HXbulLHQOhqbWDJ1Zu5/y54xmdNTLucERE+lWUeRBt4XGHmX0U2A5M7Eul7t4IFO5XVkUwqmnQePrNndS1tHNpaZ/+OUREUlKUBPF9MxsDfA34JcHEuRuSGtUg8eDyLUwqGMWJ0zTvQUSGnigT5Z4MT/cAZyQ3nMFjS3UjL66r4sazZ2vWtIgMST1NlEs4Qa5LP02UG7QeWr4VM/jECWpeEpGhqac7iGROkBvUOjudh5Zv5dSZRUzIHxV3OCIiSdHTRLl9JsiZ2eig2OuSHlWKe3lDFdt2N/ENLashIkNYlB3lSs1sFfAGsNrMXjezE5IfWup6YNkWRmeN4JyjSuIORUQkaaLMg7gb+Ed3n+ruU4BrgXuSG1bqqm1u46nVO7lw/gRtJSoiQ1qUBFHn7n/vunD3F4C0bWZ6atVOWto7ueT4CXGHIiKSVFHmQbxiZr8mWBbDgcuA583seAB3X5HE+FLOYyu3MaUwm/mT8uMORUQkqaIkiPnh8eb9yk8mSBh9XbRv0Ni5p5mXN1Rx3cJZmGnug4gMbVEmymlyXOiPr2/HHS6af3jcoYiIJF2UUUy/C5fa6LqeYmaLkxtWanps5TaOnTiG6cXaFEhEhr4ondQvAEvN7CNm9gXgWeD25IaVetaV1/Hm9lounK/OaRFJD1GamH5tZm8CzwGVwHHuvjPpkaWYx17bzjCDC+aNjzsUEZEBEaWJ6TMEcyH+AfgP4E9mNi/JcaUUd+fx17dxyswixuVlHfwFIiJDQJRRTJ8ATnX3cuA+M3sUWMTe0U1D3orNNWypbuL6M2fHHYqIyICJ0sR00X7Xr5jZguSFlHoee207mSOGce7RWlpDRNJHlCam2Wa22MxWh9fHAl9PemQporPT+fPqHZx1ZAl52lZURNJIlFFMvwG+Sbj1qLu/AVyezKBSyTvldVTWt3L6nOK4QxERGVBREkS2u7+yX1l7MoJJRS+tqwLgpBnaVlRE0kuUBFFpZjMId5czs08CO5IaVQp5aX0VUwqzmTg2O+5QREQGVJRRTNcCdwBHmNk2YCNwZVKjShHtHZ0s3VDFxzT3QUTSUJRRTBuAs8wsBxjWHzvKmVk+cCcwl+DO5PPA28D9wFSgDPiUu9f0ta6+eHN7LXUt7Zw0oyjOMEREYhGliQkAd2/ox+1Gfw485e5HAPOAt4CbgMXuPgtYHF7H6sX1lQCcNF39DyKSfiIniP4S7m19GnAXgLu3uvtu4EKCCXiEx4sSv8PAeXl9FXNK8ijOy4w7FBGRATfgCQKYDlQA95jZa2Z2Z9h8VeLuOwDC47gYYntPS3sHr5ZVa/SSiKStKBPlss3sX8zsN+H1LDP7WB/qHAEcD/zK3Y8DGjiE5iQzu8bMlpnZsoqKij6E0bOVm3fT3NbJyUoQIpKmotxB3AO0ACeF11uB7/ehzq3AVndfGl4/RJAwdpnZeIDwWJ7oxe5+h7uXuntpcXHyJq+9tL6KYQYfVP+DiKSpKAlihrvfxt6Z1E1Ar/fbDJcK32Jmc8KiM4E1wBPAVWHZVcDjva2jP7y8vopjJoxhzCgtryEi6SnKPIhWMxvF3olyMwjuKPriOuBeM8sANgCfI0hWD5jZ1cBm4NI+1tFrja3tvLalhqtPnR5XCCIisYuSIL4DPAVMMrN7gVOAz/alUndfCZQm+NKZfXnf/vJqWQ1tHa7+BxFJa1Emyj1jZsuBEwmalq5398qkRxajl9ZXMnK4UTp1bNyhiIjE5qAJwsyeAO4DnnD3huSHFL/lZTUcOzGf7IwoN1giIkNTlE7qnwAfAtaY2YNm9kkzG9L7bpZVNTBrXG7cYYiIxCpKE9MSYImZDQcWAl8g2KN6dJJji0VdcxuV9a1MLcqJOxQRkVhFakMJRzFdAFxGMGdhUc+vGLw2VTUCMLVQy3uLSHqL0gdxP/BBgpFM/w487+6dyQ4sLmVVQTfLlELdQYhIeotyB3EP8Gl370h2MKmg6w5iiu4gRCTNHTBBmNlCd/8rkA1caLbv5Gl3fyTJscViY2UDJaMzNYJJRNJeT78FPwz8laDvYX8ODMkEsamqQc1LIiL0kCDc/ebw9HvuvrH718xsWlKjilFZVSML58S60riISEqIMg/i4QRlD/V3IKmgvqWdiroWphSp/0FEpKc+iCOAo4ExZnZJty+NBobkRLlN4QimqWpiEhHpsQ9iDvAxIJ99+yHqCCbLDTl750AoQYiI9NQH8TjwuJmd5O4vD2BMsdlY2TUHQk1MIiJRxnK+ZmbXEjQ3vde05O6fT1pUMdlU1UBxXiY5mRriKiISpZP6d8BhwLnAEmAiQTPTkFNW1aglNkREQlESxEx3/xegwd0XAR8FjkluWPEoq2xQ/4OISChKgmgLj7vNbC4wBpiatIhi0tjaTnldi1ZxFREJRWlsv8PMxgL/AjwB5ALfTmpUMSir1BpMIiLdRdkP4s7wdAkwPbnhxEdzIERE9tXTRLkbe3qhu/+0/8OJT5lWcRUR2UdPdxB5AxZFCiirbKAoN4O8rJFxhyIikhJ6mij33WRVamZlBENlO4B2dy81swLgfoIO8DLgU+5ek6wY9ldWpRFMIiLdRdlR7h6C5b330Q8T5c5w98pu1zcBi939FjO7Kbz+Rh/riGxTVSOnzCwaqOpERFJelFFMT3Y7zwIuBrYnIZYLgdPD80XA8wxQgmhq7WBnbbMmyYmIdBNlFNM+y32b2X3AX/pYrwPPmJkDv3b3O4ASd98R1rnDzBJuymBm1wDXAEyePLmPYQQ2VYcjmDQHQkTkPb1ZdGgW0NffzKe4+/YwCTxrZmujvjBMJncAlJaWvq/pqze65kCoD0JEZK8ofRB1BH/xW3jcSR+bftx9e3gsN7NHgQXALjMbH949jAfK+1LHoSgL50BooyARkb0OutSGu+e5++hux9n7NzsdCjPLMbO8rnPgHGA1wSztq8KnXQU83ts6DtXm6kbGZo9ktIa4ioi8J1ITk5kdSzD89L3nu/sjvayzBHjUzLrq/727P2VmrwIPmNnVwGbg0l6+/yGrqGthXN6Q3CRPRKTXojQx3Q0cC7wJdIbFDvQqQbj7BmBegvIq4MzevGdfVTe0UpibEUfVIiIpK8odxInuflTSI4lRdUMrRx8+Ou4wRERSSpTlvl82syGdICrrWyjM0R2EiEh3Ue4gFhEkiZ1AC+FoJnc/NqmRDZDW9k7qmtspzM2MOxQRkZQSJUHcDXwGWMXePogho6axFYAC3UGIiOwjSoLY7O5PJD2SmFTVBwlCTUwiIvuKkiDWmtnvgT8SNDEBfRrmmlKqGoJvSU1MIiL7ipIgRhEkhnO6lfV6mGuqqW5QE5OISCJRFuv73EAEEhc1MYmIJBZlotw04DpCGAk+AAAMOElEQVTeP5P648kLa+BUNbQwfJgxZpSW2RAR6S5KE9NjwF0EfRBDbhRTdUMrY7MzGDbM4g5FRCSlREkQze7+i6RHEpOq+lY1L4mIJBAlQfzczG4GnmHfUUwrkhbVAKpuaFUHtYhIAlESxDEEE+UWsu9ifQuTFdRAqtI6TCIiCUVJEBcD0929NdnBxKFK6zCJiCQUZbG+14H8ZAcSh9b2Tmqb2ynI0SQ5EZH9RbmDKCGYTf0q+/ZBDPphrl3rMGkvCBGR94uSIG5OehQx0SQ5EZEDizKTeslABBIHLbMhInJgUWZS1xGMWgLIAEYCDe4+6If+7F2oTwlCRGR/Ue4g8rpfm9lFwIKkRTSA9jYxqZNaRGR/UUYx7cPdH2OIzIGobmjVOkwiIgcQpYnpkm6Xw4BS9jY59ZqZDQeWAdvc/WPhooB/AAqAFcBnkj33oqqhlbHZI7UOk4hIAlHuIC7o9jgXqAMu7Ie6rwfe6nZ9K/Azd58F1ABX90MdPapuaFHzkojIAcSyH4SZTQQ+CvwAuNHMjKDZ6tPhUxYB3wF+1d91d1dVr3WYREQO5KB3EGa2yMzyu12PNbO7+1jv7cDX2bu2UyGw293bw+utwIQ+1nFQ1Q2tFGgEk4hIQlGamI51991dF+5eAxzX2wrN7GNAubsv716c4KkJ+znM7BozW2ZmyyoqKnobBhD0QRTpDkJEJKEoCWKYmY3tujCzAqLNwD6QU4CPm1kZQaf0QoI7inwz63rficD2RC929zvcvdTdS4uLi3sdRFtHJ3ua2rQOk4jIAURJED8BXjKzfzWz7wEvAbf1tkJ3/6a7T3T3qcDlwF/d/UrgOeCT4dOuAh7vbR1R1HTNolYTk4hIQgdNEO7+W+ATwC6gArjE3X+XhFi+QdBhvY6gT+KuJNTxnqowQaiJSUQksUhNRe6+BljT35W7+/PA8+H5BgZwhrbWYRIR6dkhz6QeKirrtQ6TiEhP0jZB7L2DUCe1iEgiaZ0ghhnkax0mEZGE0jZBVIazqLUOk4hIYmmbIKobWtRBLSLSgzROEK1aqE9EpAdpmyCqtA6TiEiP0jdB1LdSqCYmEZEDSssE0bUOk5qYREQOLC0TRE2j1mESETmYtEwQVfVBglATk4jIgaVlgtA6TCIiB5eWCeK9lVzVxCQickBpmSCqw4X6tA6TiMiBpWWCODx/FOccVaJ1mEREetCXrUMHrXOOPoxzjj4s7jBERFJaWt5BiIjIwSlBiIhIQkoQIiKSkBKEiIgkpAQhIiIJKUGIiEhCShAiIpKQEoSIiCRk7h53DL1mZhXApl6+vAio7Mdw+lOqxpaqcUHqxpaqcUHqxpaqccHQiW2Kuxcf7EmDOkH0hZktc/fSuONIJFVjS9W4IHVjS9W4IHVjS9W4IP1iUxOTiIgkpAQhIiIJpXOCuCPuAHqQqrGlalyQurGlalyQurGlalyQZrGlbR+EiIj0LJ3vIEREpAdpmSDM7Dwze9vM1pnZTTHHcreZlZvZ6m5lBWb2rJm9Gx7HxhDXJDN7zszeMrM3zez6VIjNzLLM7BUzez2M67th+TQzWxrGdb+ZxbafrJkNN7PXzOzJVInNzMrMbJWZrTSzZWFZ7J+zMI58M3vIzNaGn7eT4o7NzOaE/1Zdj1oz+2rccXWL74bw87/azO4L/1/0++cs7RKEmQ0H/h04HzgKuMLMjooxpP8Aztuv7CZgsbvPAhaH1wOtHfiaux8JnAhcG/47xR1bC7DQ3ecB84HzzOxE4FbgZ2FcNcDVAxxXd9cDb3W7TpXYznD3+d2GQsb9s+zyc+Apdz8CmEfwbxdrbO7+dvhvNR84AWgEHo07LgAzmwB8BSh197nAcOBykvE5c/e0egAnAU93u/4m8M2YY5oKrO52/TYwPjwfD7ydAv9ujwNnp1JsQDawAvggwQShEYl+xgMc00SCXxwLgScBS4XYgDKgaL+y2H+WwGhgI2F/aCrF1i2Wc4AXUyUuYAKwBSgg2BX0SeDcZHzO0u4Ogr3/uF22hmWppMTddwCEx3FxBmNmU4HjgKWkQGxhE85KoBx4FlgP7Hb39vApcf5Mbwe+DnSG14WkRmwOPGNmy83smrAs9p8lMB2oAO4Jm+XuNLOcFImty+XAfeF57HG5+zbgx8BmYAewB1hOEj5n6ZggLEGZhnIdgJnlAg8DX3X32rjjAXD3Dg9u/ScCC4AjEz1tYKMCM/sYUO7uy7sXJ3hqHJ+3U9z9eIKm1WvN7LQYYkhkBHA88Ct3Pw5oIL6mrvcJ2/E/DjwYdyxdwn6PC4FpwOFADsHPdX99/pylY4LYCkzqdj0R2B5TLAeyy8zGA4TH8jiCMLORBMnhXnd/JJViA3D33cDzBH0k+WY2IvxSXD/TU4CPm1kZ8AeCZqbbUyE2d98eHssJ2tIXkBo/y63AVndfGl4/RJAwUiE2CH7xrnD3XeF1KsR1FrDR3SvcvQ14BDiZJHzO0jFBvArMCnv8MwhuH5+IOab9PQFcFZ5fRdD+P6DMzIC7gLfc/aepEpuZFZtZfng+iuA/y1vAc8An44oLwN2/6e4T3X0qwefqr+5+ZdyxmVmOmeV1nRO0qa8mBT5n7r4T2GJmc8KiM4E1qRBb6Ar2Ni9BasS1GTjRzLLD/6dd/2b9/zmLq+MnzgfwEeAdgrbrb8Ucy30E7YhtBH9NXU3Qbr0YeDc8FsQQ16kEt6hvACvDx0fijg04FngtjGs18O2wfDrwCrCOoDkgM+af6+nAk6kQW1j/6+Hjza7PfNw/y27xzQeWhT/Tx4CxqRAbwSCIKmBMt7LY4wrj+C6wNvw/8DsgMxmfM82kFhGRhNKxiUlERCJQghARkYSUIEREJCElCBERSUgJQkREElKCkJRmZm5mP+l2/U9m9p0k1POjcHXMH/X3e6cSM5tqZp+OOw4ZHJQgJNW1AJeYWVGS6/lfwPHu/s9JriduUwElCIlECUJSXTvBVoo37P8FM5tiZovN7I3wOLmnN7LAj8I19FeZ2WVh+RME69ks7Srr9ppcM7snfP4bZvaJsPyKsGy1md3a7fn1ZnZruCjeX8xsgZk9b2YbzOzj4XM+a2aPm9lTFuxLcnO3198YvudqM/tqWDbVgn0SfhPe5TwTziLHzGaE77PczP5uZkeE5f9hZr8ws5fCurtm2N4CfMiCPQ5uMLOjLdhfY2X4/c06tB+PDGlxzALUQ4+oD6CeYEnoMmAM8E/Ad8Kv/RG4Kjz/PPDYQd7rEwSrvw4HSgiWLOhaurn+AK+5Fbi92/VYggXSNgPFBIvN/RW4KPy6A+eH548CzwAjCfY5WBmWf5Zg9nwhMIpgNmwpwb4DqwiSVS7BrOfjCP7qbwfmh69/APgf4fliYFZ4/kGC5T0g2GfkQYI/Ao8C1oXlpxPO8A6vfwlcGZ5nAKPi/pnrkTqProWdRFKWu9ea2W8JNklp6valk4BLwvPfAbcd5K1OBe5z9w6CRdeWAB+g57W4ziJYV6krlppwJdTn3b0CwMzuBU4jWCaiFXgqfPoqoMXd28xsFcEv+i7PuntV+PpH2Lu0yaPu3tCt/ENhfBvdfWX42uXA1HCl3ZOBB4MleYBgyYUuj7l7J7DGzEoO8P29DHzLzCYCj7j7uz38W0iaUROTDBa3E6xTldPDcw62bkyipbcPxhK8b0/v0+buXc/vJOhDIfxF3f0Psv3f0w/yvi3dzjvC9xpGsAfA/G6PIw/wmoTv7e6/J1jOugl42swW9hCDpBklCBkU3L2aoGml+zaKL7H3r/srgRcO8jZ/Ay4LNxwqJvir/5WDvOYZ4MtdF+Fa/EuBD5tZkQVb2F4BLIn6vYTOtmB/41HARcCLYXwXhat05gAXA38/0Bt4sD/HRjO7NIzNzGzeQeqtA/K6fT/TgQ3u/guCO5VjD/H7kCFMCUIGk58A3UczfQX4nJm9AXyGYC9ozOzjZva9BK9/lGDF0NcJ+g2+7sFy0z35PjA27DR+nWBf5x0EW9U+F77XCnc/1KWVXyBoFlsJPOzuy9x9BUHfwSsESehOd3/tIO9zJXB1GNubBBvJ9OQNoN3MXjezG4DLgNUW7NB3BPDbQ/w+ZAjTaq4iA8zMPkuw4fyXD/ZckTjpDkJERBLSHYSIiCSkOwgREUlICUJERBJSghARkYSUIEREJCElCBERSUgJQkREEvr/Zt2BeqgxIrcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_test = pca.transform(X_test2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train2int, y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.91      0.87       500\n",
      "          1       0.84      0.78      0.81       500\n",
      "          2       0.43      0.69      0.53       500\n",
      "          3       0.99      0.98      0.99       500\n",
      "          4       0.92      0.90      0.91       500\n",
      "          5       0.81      0.75      0.78       500\n",
      "          6       0.80      0.50      0.61       500\n",
      "          7       0.74      0.80      0.77       500\n",
      "          8       0.36      0.85      0.50       500\n",
      "          9       0.46      0.75      0.57       500\n",
      "         10       0.61      0.76      0.68       500\n",
      "         11       0.98      0.86      0.92       500\n",
      "         12       0.84      0.86      0.85       500\n",
      "         13       0.94      0.99      0.96       500\n",
      "         14       0.88      0.93      0.90       500\n",
      "         15       0.82      0.37      0.51       500\n",
      "         16       0.88      0.54      0.67       500\n",
      "         17       0.82      0.08      0.15       500\n",
      "         18       0.64      0.28      0.39       500\n",
      "         19       0.90      0.04      0.07       500\n",
      "         20       0.78      0.68      0.73       500\n",
      "         21       0.75      0.60      0.67       500\n",
      "         22       0.98      0.99      0.99       500\n",
      "         23       0.88      0.10      0.18       500\n",
      "         24       0.54      0.98      0.69       500\n",
      "         25       0.86      0.90      0.88       500\n",
      "         26       0.94      0.72      0.81       500\n",
      "         27       0.70      0.87      0.77       500\n",
      "         28       0.62      0.60      0.61       500\n",
      "         29       0.53      0.15      0.23       500\n",
      "         30       0.99      0.95      0.97       500\n",
      "         31       0.91      0.70      0.79       500\n",
      "         32       0.91      0.95      0.93       500\n",
      "         33       0.89      0.96      0.92       500\n",
      "         34       0.99      0.99      0.99       500\n",
      "         35       0.57      0.96      0.71       500\n",
      "         36       0.70      0.99      0.82       500\n",
      "         37       0.90      0.90      0.90       500\n",
      "         38       0.88      0.60      0.71       500\n",
      "         39       0.95      0.83      0.89       500\n",
      "         40       1.00      0.95      0.97       500\n",
      "         41       0.88      0.98      0.93       500\n",
      "         42       0.45      0.91      0.60       500\n",
      "         43       0.57      0.67      0.62       500\n",
      "         44       0.97      0.85      0.91       500\n",
      "         45       0.89      1.00      0.94       500\n",
      "         46       0.97      0.80      0.88       500\n",
      "         47       0.00      0.00      0.00       500\n",
      "         48       0.88      0.95      0.92       500\n",
      "         49       0.86      0.99      0.92       500\n",
      "         50       0.34      0.55      0.42       500\n",
      "         51       0.87      0.78      0.82       500\n",
      "         52       0.70      0.72      0.71       500\n",
      "         53       0.80      0.97      0.88       500\n",
      "         54       0.93      0.56      0.70       500\n",
      "         55       0.97      0.42      0.59       500\n",
      "         56       0.68      0.87      0.76       500\n",
      "         57       0.79      0.96      0.87       500\n",
      "         58       0.78      0.39      0.52       500\n",
      "         59       0.86      0.76      0.81       500\n",
      "         60       0.84      0.89      0.86       500\n",
      "         61       0.99      0.83      0.90       500\n",
      "         62       0.94      0.69      0.80       500\n",
      "         63       0.93      0.95      0.94       500\n",
      "         64       0.93      0.96      0.95       500\n",
      "         65       0.90      0.71      0.79       500\n",
      "         66       0.95      0.36      0.52       500\n",
      "         67       0.78      0.97      0.87       500\n",
      "         68       0.92      0.98      0.95       500\n",
      "         69       0.98      0.94      0.96       500\n",
      "         70       0.94      0.94      0.94       500\n",
      "         71       0.76      0.99      0.86       500\n",
      "         72       0.85      0.94      0.89       500\n",
      "         73       0.30      0.66      0.41       500\n",
      "         74       0.99      0.30      0.46       500\n",
      "         75       0.55      0.83      0.66       500\n",
      "         76       0.71      0.05      0.09       500\n",
      "         77       0.67      0.00      0.01       500\n",
      "         78       0.87      0.88      0.88       500\n",
      "         79       0.95      0.96      0.95       500\n",
      "         80       0.96      0.97      0.97       500\n",
      "         81       0.96      0.71      0.82       500\n",
      "         82       0.74      0.62      0.68       500\n",
      "         83       0.67      0.60      0.63       500\n",
      "         84       0.85      0.86      0.86       500\n",
      "         85       0.82      0.65      0.73       500\n",
      "         86       0.54      0.20      0.30       500\n",
      "         87       0.66      0.99      0.79       500\n",
      "         88       0.65      0.76      0.70       500\n",
      "         89       0.83      0.93      0.88       500\n",
      "         90       0.74      0.51      0.60       500\n",
      "         91       0.91      1.00      0.95       500\n",
      "         92       0.93      0.98      0.96       500\n",
      "         93       0.86      0.96      0.91       500\n",
      "         94       0.84      0.91      0.88       500\n",
      "         95       0.50      1.00      0.66       500\n",
      "         96       0.99      0.48      0.65       500\n",
      "         97       0.93      0.98      0.96       500\n",
      "         98       0.73      0.97      0.84       500\n",
      "         99       0.98      0.88      0.93       500\n",
      "        100       0.89      0.84      0.86       500\n",
      "        101       1.00      0.04      0.07       500\n",
      "        102       0.18      0.56      0.27       500\n",
      "        103       0.81      0.32      0.46       500\n",
      "        104       0.63      0.57      0.60       500\n",
      "        105       0.99      0.99      0.99       500\n",
      "        106       0.95      0.91      0.93       500\n",
      "        107       0.86      0.90      0.88       500\n",
      "        108       0.90      0.98      0.93       500\n",
      "        109       0.64      0.89      0.74       500\n",
      "        110       0.98      0.85      0.91       500\n",
      "        111       0.73      0.79      0.76       500\n",
      "        112       0.86      0.97      0.91       500\n",
      "        113       0.99      0.83      0.91       500\n",
      "        114       0.98      0.80      0.88       500\n",
      "        115       0.94      0.54      0.68       500\n",
      "        116       0.90      0.85      0.88       500\n",
      "        117       0.65      0.94      0.77       500\n",
      "        118       0.65      0.82      0.73       500\n",
      "        119       0.84      0.61      0.71       500\n",
      "        120       0.99      0.86      0.92       500\n",
      "        121       0.88      0.60      0.71       500\n",
      "        122       0.96      0.92      0.94       500\n",
      "        123       0.65      0.96      0.78       500\n",
      "        124       0.96      0.21      0.34       500\n",
      "        125       0.98      0.99      0.98       500\n",
      "        126       0.68      0.26      0.38       500\n",
      "        127       0.54      0.24      0.34       500\n",
      "        128       0.96      0.53      0.69       500\n",
      "        129       0.57      0.78      0.65       500\n",
      "        130       0.70      0.97      0.81       500\n",
      "        131       0.94      0.24      0.38       500\n",
      "        132       0.90      0.99      0.95       500\n",
      "        133       0.82      0.97      0.89       500\n",
      "        134       0.94      0.67      0.78       500\n",
      "        135       0.86      0.99      0.92       500\n",
      "        136       0.88      0.97      0.92       500\n",
      "        137       0.51      0.46      0.48       500\n",
      "        138       0.84      0.82      0.83       500\n",
      "        139       1.00      1.00      1.00       500\n",
      "        140       0.44      0.67      0.53       500\n",
      "        141       0.70      0.86      0.77       500\n",
      "        142       0.96      0.95      0.96       500\n",
      "        143       0.72      0.74      0.73       500\n",
      "        144       1.00      1.00      1.00       500\n",
      "        145       0.69      0.64      0.66       500\n",
      "        146       0.68      0.80      0.74       500\n",
      "        147       0.86      0.26      0.40       500\n",
      "        148       0.26      0.95      0.41       500\n",
      "        149       0.83      0.91      0.87       500\n",
      "        150       0.78      0.56      0.65       500\n",
      "        151       0.52      0.52      0.52       500\n",
      "        152       0.77      0.28      0.41       500\n",
      "        153       0.72      0.88      0.79       500\n",
      "        154       0.78      0.90      0.84       500\n",
      "        155       0.90      0.35      0.50       500\n",
      "        156       0.90      0.75      0.82       500\n",
      "        157       0.98      0.96      0.97       500\n",
      "        158       0.92      0.92      0.92       500\n",
      "        159       0.70      0.99      0.82       500\n",
      "        160       0.78      0.73      0.75       500\n",
      "        161       0.54      0.04      0.07       500\n",
      "        162       0.94      0.99      0.97       500\n",
      "        163       0.88      0.43      0.57       500\n",
      "        164       0.66      0.57      0.61       500\n",
      "        165       0.76      0.40      0.52       500\n",
      "        166       0.69      0.87      0.77       500\n",
      "        167       0.77      0.88      0.82       500\n",
      "        168       0.89      0.98      0.93       500\n",
      "        169       0.92      0.67      0.78       500\n",
      "        170       0.86      0.84      0.85       500\n",
      "        171       0.88      0.93      0.90       500\n",
      "        172       0.61      0.92      0.73       500\n",
      "        173       0.82      0.88      0.85       500\n",
      "        174       0.79      0.98      0.87       500\n",
      "        175       0.99      0.14      0.24       500\n",
      "        176       0.93      0.84      0.88       500\n",
      "        177       0.90      0.18      0.29       500\n",
      "        178       0.96      0.90      0.92       500\n",
      "        179       0.99      0.68      0.81       500\n",
      "        180       0.77      0.86      0.81       500\n",
      "        181       0.67      0.36      0.47       500\n",
      "        182       0.98      0.99      0.98       500\n",
      "        183       0.97      0.97      0.97       500\n",
      "        184       0.91      0.88      0.89       500\n",
      "        185       0.84      0.67      0.74       500\n",
      "        186       0.84      0.87      0.85       500\n",
      "        187       0.74      0.88      0.81       500\n",
      "        188       0.85      0.98      0.91       500\n",
      "        189       0.97      0.91      0.94       500\n",
      "        190       0.35      0.78      0.48       500\n",
      "        191       0.75      0.95      0.84       500\n",
      "        192       0.62      0.93      0.74       500\n",
      "        193       0.74      0.92      0.82       500\n",
      "        194       0.49      0.92      0.64       500\n",
      "        195       0.65      0.93      0.77       500\n",
      "        196       0.53      0.88      0.66       500\n",
      "        197       0.98      0.82      0.89       500\n",
      "        198       0.89      0.84      0.86       500\n",
      "        199       0.94      0.97      0.96       500\n",
      "        200       0.98      0.99      0.99       500\n",
      "        201       0.57      0.84      0.68       500\n",
      "        202       0.00      0.00      0.00       500\n",
      "        203       0.99      0.96      0.97       500\n",
      "        204       0.57      0.81      0.67       500\n",
      "        205       0.99      0.92      0.95       500\n",
      "        206       0.73      0.78      0.75       500\n",
      "        207       0.89      0.99      0.94       500\n",
      "        208       0.98      0.98      0.98       500\n",
      "        209       0.85      0.81      0.83       500\n",
      "        210       0.95      0.98      0.97       500\n",
      "        211       0.83      0.89      0.86       500\n",
      "        212       0.70      0.87      0.78       500\n",
      "        213       1.00      0.09      0.16       500\n",
      "        214       0.93      0.96      0.95       500\n",
      "        215       0.70      0.97      0.82       500\n",
      "        216       0.93      0.52      0.66       500\n",
      "        217       0.81      0.99      0.89       500\n",
      "        218       0.84      0.88      0.86       500\n",
      "        219       0.91      0.88      0.90       500\n",
      "        220       0.97      0.98      0.98       500\n",
      "        221       0.48      0.76      0.59       500\n",
      "        222       0.87      0.99      0.93       500\n",
      "        223       0.90      0.95      0.92       500\n",
      "        224       0.52      0.99      0.68       500\n",
      "        225       0.84      0.95      0.89       500\n",
      "        226       0.79      0.92      0.85       500\n",
      "        227       0.84      0.71      0.77       500\n",
      "        228       0.94      0.92      0.93       500\n",
      "        229       0.67      0.99      0.80       500\n",
      "        230       0.58      0.98      0.73       500\n",
      "        231       0.88      0.75      0.81       500\n",
      "        232       0.77      0.46      0.58       500\n",
      "        233       0.81      0.50      0.62       500\n",
      "        234       0.81      0.79      0.80       500\n",
      "\n",
      "avg / total       0.80      0.76      0.75    117500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_int, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622978723404256"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_train_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = np.array(y_train_int).reshape(-1, 1)\n",
    "y_test_int = np.array(y_test_int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = enc.transform(y_train_int)\n",
    "y_test_enc = enc.transform(y_test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", input_dim=80, units=130)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=130)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"relu\", units=130)`\n",
      "  \"\"\"\n",
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"uniform\", activation=\"softmax\", units=235)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "clf.add(Dense(output_dim = 130, kernel_initializer = 'uniform', activation='relu', input_dim = 80))\n",
    "clf.add(Dropout(rate=0.2))\n",
    "clf.add(Dense(output_dim = 130, kernel_initializer = 'uniform', activation='relu'))\n",
    "clf.add(Dropout(rate=0.3))\n",
    "clf.add(Dense(output_dim = 130, kernel_initializer = 'uniform', activation='relu'))\n",
    "clf.add(Dropout(rate=0.2))\n",
    "clf.add(Dense(output_dim =235  , kernel_initializer = 'uniform', activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117500 samples, validate on 117500 samples\n",
      "Epoch 1/10\n",
      "117500/117500 [==============================] - 25s 210us/step - loss: 2.9933 - acc: 0.2187 - val_loss: 1.9743 - val_acc: 0.4265\n",
      "Epoch 2/10\n",
      "117500/117500 [==============================] - 13s 112us/step - loss: 2.0317 - acc: 0.3985 - val_loss: 1.5655 - val_acc: 0.5292\n",
      "Epoch 3/10\n",
      "117500/117500 [==============================] - 15s 128us/step - loss: 1.6723 - acc: 0.4957 - val_loss: 1.2570 - val_acc: 0.6277\n",
      "Epoch 4/10\n",
      "117500/117500 [==============================] - 15s 127us/step - loss: 1.4185 - acc: 0.5723 - val_loss: 1.0623 - val_acc: 0.6867\n",
      "Epoch 5/10\n",
      "117500/117500 [==============================] - 14s 119us/step - loss: 1.2579 - acc: 0.6230 - val_loss: 0.9500 - val_acc: 0.7263\n",
      "Epoch 6/10\n",
      "117500/117500 [==============================] - 14s 120us/step - loss: 1.1487 - acc: 0.6562 - val_loss: 0.8634 - val_acc: 0.7524\n",
      "Epoch 7/10\n",
      "117500/117500 [==============================] - 14s 122us/step - loss: 1.0712 - acc: 0.6810 - val_loss: 0.8115 - val_acc: 0.7610\n",
      "Epoch 8/10\n",
      "117500/117500 [==============================] - 13s 115us/step - loss: 1.0172 - acc: 0.6978 - val_loss: 0.7755 - val_acc: 0.7791\n",
      "Epoch 9/10\n",
      "117500/117500 [==============================] - 13s 115us/step - loss: 0.9743 - acc: 0.7125 - val_loss: 0.7404 - val_acc: 0.7900\n",
      "Epoch 10/10\n",
      "117500/117500 [==============================] - 14s 115us/step - loss: 0.9388 - acc: 0.7247 - val_loss: 0.7177 - val_acc: 0.7942\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    clf.fit(pca_X_train, y_train_enc, batch_size=48, epochs=10, validation_data=(pca_X_test,y_test_enc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
