{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_fwf('./wili dataset/x_train.txt', header=None)\n",
    "X_train = df[[0]]\n",
    "df = pd.read_fwf('./wili dataset/x_test.txt', header=None)\n",
    "X_test = df[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_fwf('./wili dataset/y_train.txt',header = None)\n",
    "y_train = target[0]\n",
    "target = pd.read_fwf('./wili dataset/y_test.txt',header = None)\n",
    "y_test = target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Klement Gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Klement Gottwaldi surnukeha palsameeriti ning ...\n",
       "1  Sebes, Joseph; Pereira Thomas (1961) (pÃ¥ eng)...\n",
       "2  à¤­à¤¾à¤°à¤¤à¥€à¤¯ à¤¸à¥�à¤µà¤¾à¤¤à¤¨à¥�à¤¤à¥�...\n",
       "3  AprÃ¨s lo cort periÃ²de d'establiment a BasilÃ...\n",
       "4  à¸–à¸™à¸™à¹€à¸ˆà¸£à¸´à¸�à¸�à¸£à¸¸à¸‡ (à¸­à¸±à¸..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ne l fin de l seclo XIX l Japon era inda Ã§con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiedam is gelegen tussen Rotterdam en Vlaard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Halukum adalah kelenjar tiroid nang menonjol d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Ne l fin de l seclo XIX l Japon era inda Ã§con...\n",
       "1  Schiedam is gelegen tussen Rotterdam en Vlaard...\n",
       "2  Ð“IÑƒÑ€ÑƒÑ�Ð°Ð· Ð±Ð°Ñ‚Ð°Ð»ÑŒÐ¾Ð½Ð°Ð», Ð³ÑŒÐ¾Ñ€...\n",
       "3  à²°à²¾à²œà³�à²¯à²¶à²¾à²¸à³�à²¤à³�à²°à²¦ à²ªà²¿...\n",
       "4  Halukum adalah kelenjar tiroid nang menonjol d..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    est\n",
       "1    swe\n",
       "2    mai\n",
       "3    oci\n",
       "4    tha\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[y_train == 'be-tara'] = 'be-tarask'\n",
    "y_train[y_train == 'roa-tar'] = 'roa-tara'\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./wili dataset/labels.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>English</th>\n",
       "      <th>Wiki Code</th>\n",
       "      <th>ISO 369-3</th>\n",
       "      <th>German</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Writing system</th>\n",
       "      <th>Remarks</th>\n",
       "      <th>Synonyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ace</td>\n",
       "      <td>Achinese</td>\n",
       "      <td>ace</td>\n",
       "      <td>ace</td>\n",
       "      <td>Achinesisch</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>af</td>\n",
       "      <td>afr</td>\n",
       "      <td>Afrikaans</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>als</td>\n",
       "      <td>Alemannic German</td>\n",
       "      <td>als</td>\n",
       "      <td>gsw</td>\n",
       "      <td>Alemannisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ursprünglich nur Elsässisch)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amh</td>\n",
       "      <td>Amharic</td>\n",
       "      <td>am</td>\n",
       "      <td>amh</td>\n",
       "      <td>Amharisch</td>\n",
       "      <td>Afro-Asiatic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ang</td>\n",
       "      <td>Old English</td>\n",
       "      <td>ang</td>\n",
       "      <td>ang</td>\n",
       "      <td>Altenglisch</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(ca. 450-1100)</td>\n",
       "      <td>Angelsächsisch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label           English Wiki Code ISO 369-3       German Language family  \\\n",
       "0   ace          Achinese       ace       ace  Achinesisch    Austronesian   \n",
       "1   afr         Afrikaans        af       afr    Afrikaans   Indo-European   \n",
       "2   als  Alemannic German       als       gsw  Alemannisch   Indo-European   \n",
       "3   amh           Amharic        am       amh    Amharisch    Afro-Asiatic   \n",
       "4   ang      Old English        ang       ang  Altenglisch   Indo-European   \n",
       "\n",
       "  Writing system                        Remarks        Synonyms  \n",
       "0            NaN                            NaN             NaN  \n",
       "1            NaN                            NaN             NaN  \n",
       "2            NaN  (ursprünglich nur Elsässisch)             NaN  \n",
       "3            NaN                            NaN             NaN  \n",
       "4            NaN                 (ca. 450-1100)  Angelsächsisch  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {}\n",
    "counter = 0\n",
    "for label in labels['Label']:\n",
    "    if label not in label2int:\n",
    "        label2int[label] = counter\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the  target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = []\n",
    "for label in y_train:\n",
    "    y_train_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_int = []\n",
    "for label in y_test:\n",
    "    y_test_int.append(label2int[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char',min_df=25,lowercase=True, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=25,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2int = vectorizer.transform(X_train[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2int = vectorizer.transform(X_test[0]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117500, 155)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=80, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_train = pca.transform(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_X_test = pca.transform(X_test2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train2int, y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.91      0.85       500\n",
      "          1       0.95      0.63      0.76       500\n",
      "          2       0.78      0.56      0.65       500\n",
      "          3       0.99      0.98      0.99       500\n",
      "          4       0.95      0.89      0.92       500\n",
      "          5       0.79      0.78      0.78       500\n",
      "          6       0.79      0.38      0.51       500\n",
      "          7       0.63      0.75      0.68       500\n",
      "          8       0.87      0.80      0.84       500\n",
      "          9       0.22      0.89      0.36       500\n",
      "         10       0.98      0.62      0.76       500\n",
      "         11       0.99      0.85      0.91       500\n",
      "         12       0.71      0.93      0.81       500\n",
      "         13       0.94      0.99      0.96       500\n",
      "         14       0.72      0.96      0.82       500\n",
      "         15       0.50      0.61      0.55       500\n",
      "         16       0.67      0.73      0.70       500\n",
      "         17       0.49      0.93      0.64       500\n",
      "         18       0.00      0.00      0.00       500\n",
      "         19       0.85      0.86      0.86       500\n",
      "         20       0.43      0.89      0.58       500\n",
      "         21       0.87      0.56      0.68       500\n",
      "         22       0.99      0.99      0.99       500\n",
      "         23       0.37      0.73      0.49       500\n",
      "         24       0.93      0.91      0.92       500\n",
      "         25       0.95      0.85      0.89       500\n",
      "         26       0.76      0.34      0.47       500\n",
      "         27       0.92      0.77      0.84       500\n",
      "         28       0.94      0.25      0.39       500\n",
      "         29       0.92      0.16      0.28       500\n",
      "         30       0.99      0.95      0.97       500\n",
      "         31       0.80      0.76      0.78       500\n",
      "         32       0.93      0.94      0.94       500\n",
      "         33       0.98      0.92      0.95       500\n",
      "         34       0.99      0.99      0.99       500\n",
      "         35       0.79      0.95      0.86       500\n",
      "         36       0.91      0.99      0.95       500\n",
      "         37       0.84      0.93      0.88       500\n",
      "         38       0.97      0.45      0.61       500\n",
      "         39       0.87      0.89      0.88       500\n",
      "         40       0.99      0.95      0.97       500\n",
      "         41       0.91      0.98      0.94       500\n",
      "         42       0.80      0.73      0.76       500\n",
      "         43       0.73      0.49      0.59       500\n",
      "         44       0.90      0.86      0.88       500\n",
      "         45       0.85      1.00      0.92       500\n",
      "         46       0.77      0.91      0.83       500\n",
      "         47       0.60      0.18      0.27       500\n",
      "         48       0.92      0.94      0.93       500\n",
      "         49       0.87      0.99      0.93       500\n",
      "         50       0.26      0.77      0.39       500\n",
      "         51       0.76      0.83      0.79       500\n",
      "         52       0.75      0.69      0.72       500\n",
      "         53       0.75      0.97      0.85       500\n",
      "         54       0.81      0.62      0.70       500\n",
      "         55       0.87      0.78      0.82       500\n",
      "         56       0.78      0.48      0.59       500\n",
      "         57       0.87      0.95      0.91       500\n",
      "         58       0.77      0.02      0.04       500\n",
      "         59       0.90      0.75      0.82       500\n",
      "         60       0.86      0.90      0.88       500\n",
      "         61       0.85      0.91      0.88       500\n",
      "         62       0.90      0.74      0.81       500\n",
      "         63       0.85      0.95      0.90       500\n",
      "         64       0.65      0.98      0.78       500\n",
      "         65       0.94      0.45      0.61       500\n",
      "         66       0.57      0.56      0.56       500\n",
      "         67       0.85      0.96      0.90       500\n",
      "         68       0.91      0.98      0.94       500\n",
      "         69       0.89      0.95      0.92       500\n",
      "         70       0.95      0.94      0.94       500\n",
      "         71       0.78      0.99      0.87       500\n",
      "         72       0.91      0.88      0.90       500\n",
      "         73       0.75      0.02      0.05       500\n",
      "         74       0.50      1.00      0.66       500\n",
      "         75       0.93      0.72      0.81       500\n",
      "         76       0.87      0.36      0.51       500\n",
      "         77       0.48      0.03      0.05       500\n",
      "         78       0.95      0.70      0.80       500\n",
      "         79       0.94      0.96      0.95       500\n",
      "         80       0.94      0.97      0.96       500\n",
      "         81       0.81      0.75      0.78       500\n",
      "         82       0.77      0.65      0.71       500\n",
      "         83       0.70      0.58      0.63       500\n",
      "         84       0.83      0.86      0.84       500\n",
      "         85       0.88      0.60      0.72       500\n",
      "         86       0.53      0.37      0.44       500\n",
      "         87       0.85      0.97      0.90       500\n",
      "         88       0.83      0.45      0.58       500\n",
      "         89       0.96      0.85      0.90       500\n",
      "         90       0.87      0.57      0.69       500\n",
      "         91       0.90      1.00      0.95       500\n",
      "         92       0.94      0.98      0.96       500\n",
      "         93       0.94      0.95      0.94       500\n",
      "         94       0.85      0.91      0.88       500\n",
      "         95       1.00      0.01      0.03       500\n",
      "         96       0.00      0.00      0.00       500\n",
      "         97       0.84      0.98      0.91       500\n",
      "         98       0.75      0.98      0.85       500\n",
      "         99       0.98      0.88      0.93       500\n",
      "        100       0.88      0.81      0.85       500\n",
      "        101       1.00      0.21      0.35       500\n",
      "        102       0.83      0.15      0.25       500\n",
      "        103       0.88      0.42      0.57       500\n",
      "        104       0.70      0.43      0.54       500\n",
      "        105       0.99      0.99      0.99       500\n",
      "        106       0.51      0.97      0.67       500\n",
      "        107       0.89      0.88      0.89       500\n",
      "        108       0.88      0.98      0.93       500\n",
      "        109       0.93      0.75      0.83       500\n",
      "        110       0.99      0.85      0.91       500\n",
      "        111       0.68      0.83      0.75       500\n",
      "        112       0.86      0.95      0.91       500\n",
      "        113       0.97      0.90      0.93       500\n",
      "        114       0.80      0.89      0.84       500\n",
      "        115       0.90      0.54      0.68       500\n",
      "        116       0.83      0.86      0.85       500\n",
      "        117       0.93      0.89      0.91       500\n",
      "        118       0.90      0.69      0.78       500\n",
      "        119       0.60      0.75      0.67       500\n",
      "        120       0.79      0.93      0.85       500\n",
      "        121       0.74      0.74      0.74       500\n",
      "        122       0.77      0.95      0.85       500\n",
      "        123       0.73      0.95      0.83       500\n",
      "        124       0.27      0.90      0.41       500\n",
      "        125       0.98      0.99      0.99       500\n",
      "        126       0.62      0.38      0.47       500\n",
      "        127       0.81      0.28      0.42       500\n",
      "        128       0.63      0.94      0.75       500\n",
      "        129       0.94      0.55      0.69       500\n",
      "        130       0.79      0.87      0.83       500\n",
      "        131       0.41      0.96      0.57       500\n",
      "        132       0.75      0.99      0.86       500\n",
      "        133       0.90      0.97      0.94       500\n",
      "        134       0.91      0.88      0.89       500\n",
      "        135       0.91      0.99      0.95       500\n",
      "        136       0.59      0.98      0.74       500\n",
      "        137       0.58      0.44      0.50       500\n",
      "        138       0.68      0.86      0.76       500\n",
      "        139       1.00      1.00      1.00       500\n",
      "        140       0.35      0.24      0.28       500\n",
      "        141       0.99      0.28      0.43       500\n",
      "        142       0.96      0.96      0.96       500\n",
      "        143       0.74      0.76      0.75       500\n",
      "        144       1.00      1.00      1.00       500\n",
      "        145       0.86      0.62      0.72       500\n",
      "        146       0.98      0.49      0.65       500\n",
      "        147       0.26      0.85      0.40       500\n",
      "        148       0.60      0.10      0.17       500\n",
      "        149       0.86      0.90      0.88       500\n",
      "        150       0.49      0.58      0.53       500\n",
      "        151       0.91      0.20      0.32       500\n",
      "        152       0.79      0.39      0.53       500\n",
      "        153       0.92      0.79      0.85       500\n",
      "        154       0.83      0.88      0.86       500\n",
      "        155       0.33      0.69      0.45       500\n",
      "        156       0.93      0.76      0.83       500\n",
      "        157       0.97      0.96      0.96       500\n",
      "        158       0.83      0.90      0.87       500\n",
      "        159       0.86      0.99      0.92       500\n",
      "        160       0.87      0.69      0.77       500\n",
      "        161       0.31      0.60      0.41       500\n",
      "        162       0.96      0.99      0.98       500\n",
      "        163       0.99      0.33      0.49       500\n",
      "        164       0.40      0.74      0.52       500\n",
      "        165       0.59      0.57      0.58       500\n",
      "        166       0.76      0.85      0.80       500\n",
      "        167       0.61      0.96      0.75       500\n",
      "        168       0.93      0.97      0.95       500\n",
      "        169       0.69      0.83      0.75       500\n",
      "        170       0.86      0.79      0.83       500\n",
      "        171       0.88      0.93      0.90       500\n",
      "        172       0.90      0.85      0.87       500\n",
      "        173       0.80      0.87      0.83       500\n",
      "        174       0.87      0.97      0.92       500\n",
      "        175       0.97      0.08      0.14       500\n",
      "        176       0.89      0.85      0.87       500\n",
      "        177       0.68      0.22      0.33       500\n",
      "        178       0.92      0.89      0.90       500\n",
      "        179       1.00      0.48      0.65       500\n",
      "        180       0.73      0.84      0.78       500\n",
      "        181       0.56      0.42      0.48       500\n",
      "        182       0.96      0.99      0.98       500\n",
      "        183       0.97      0.97      0.97       500\n",
      "        184       0.81      0.89      0.85       500\n",
      "        185       0.86      0.60      0.70       500\n",
      "        186       0.72      0.87      0.79       500\n",
      "        187       0.86      0.86      0.86       500\n",
      "        188       0.90      0.97      0.93       500\n",
      "        189       0.92      0.93      0.92       500\n",
      "        190       0.84      0.20      0.33       500\n",
      "        191       0.88      0.94      0.91       500\n",
      "        192       0.87      0.88      0.88       500\n",
      "        193       0.75      0.91      0.83       500\n",
      "        194       0.96      0.54      0.69       500\n",
      "        195       0.69      0.92      0.79       500\n",
      "        196       0.71      0.84      0.77       500\n",
      "        197       0.93      0.88      0.91       500\n",
      "        198       0.65      0.90      0.76       500\n",
      "        199       0.92      0.98      0.95       500\n",
      "        200       0.98      0.99      0.99       500\n",
      "        201       0.97      0.59      0.74       500\n",
      "        202       0.50      1.00      0.67       500\n",
      "        203       0.99      0.96      0.97       500\n",
      "        204       0.92      0.72      0.81       500\n",
      "        205       0.90      0.97      0.93       500\n",
      "        206       0.94      0.47      0.62       500\n",
      "        207       0.89      0.99      0.94       500\n",
      "        208       0.98      0.98      0.98       500\n",
      "        209       0.86      0.84      0.85       500\n",
      "        210       0.92      0.99      0.95       500\n",
      "        211       0.87      0.88      0.88       500\n",
      "        212       0.92      0.62      0.74       500\n",
      "        213       0.67      0.82      0.73       500\n",
      "        214       0.77      0.96      0.86       500\n",
      "        215       0.38      0.99      0.55       500\n",
      "        216       0.93      0.30      0.45       500\n",
      "        217       0.84      0.98      0.91       500\n",
      "        218       0.83      0.88      0.85       500\n",
      "        219       0.87      0.87      0.87       500\n",
      "        220       0.97      0.98      0.98       500\n",
      "        221       0.81      0.59      0.68       500\n",
      "        222       0.91      0.98      0.94       500\n",
      "        223       0.84      0.95      0.89       500\n",
      "        224       0.71      0.98      0.82       500\n",
      "        225       0.90      0.95      0.92       500\n",
      "        226       0.71      0.92      0.80       500\n",
      "        227       0.89      0.61      0.72       500\n",
      "        228       0.96      0.88      0.92       500\n",
      "        229       0.50      0.99      0.66       500\n",
      "        230       0.00      0.00      0.00       500\n",
      "        231       0.86      0.74      0.79       500\n",
      "        232       0.90      0.37      0.52       500\n",
      "        233       0.87      0.46      0.60       500\n",
      "        234       0.64      0.90      0.75       500\n",
      "\n",
      "avg / total       0.80      0.76      0.74    117500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jatin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train_int, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7574808510638298"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_train_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_int = np.array(y_train_int).reshape(-1, 1)\n",
    "y_test_int = np.array(y_test_int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y_train_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_enc = enc.transform(y_train_int)\n",
    "y_test_enc = enc.transform(y_test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
